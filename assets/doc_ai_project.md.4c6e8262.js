import{_ as e,c as r,o as t,a}from"./app.9e6df08e.js";const d=JSON.parse('{"title":"AI 辅助学习","description":"","frontmatter":{},"headers":[],"relativePath":"doc/ai/project.md","lastUpdated":1680433625000}'),o={name:"doc/ai/project.md"},i=a('<h1 id="ai-辅助学习" tabindex="-1">AI 辅助学习 <a class="header-anchor" href="#ai-辅助学习" aria-hidden="true">#</a></h1><hr><h1 id="stable-diffusion-中使用到的技术对应的说明" tabindex="-1">stable-diffusion 中使用到的技术对应的说明 <a class="header-anchor" href="#stable-diffusion-中使用到的技术对应的说明" aria-hidden="true">#</a></h1><h2 id="https-github-com-stability-ai-stablediffusion" tabindex="-1"><a href="https://github.com/Stability-AI/stablediffusion" target="_blank" rel="noreferrer">https://github.com/Stability-AI/stablediffusion</a> <a class="header-anchor" href="#https-github-com-stability-ai-stablediffusion" aria-hidden="true">#</a></h2><p>这个项目是一个高分辨率图像合成的开源实现，基于潜在扩散模型，参考了论文《High-Resolution Image Synthesis with Latent Diffusion Models》¹。该项目提供了多种基于扩散的图像生成模型，包括文本引导的图像生成、超分辨率放大、深度引导的图像生成等。该项目使用了OpenCLIP作为文本编码器，并在LAION-5B数据集上进行了训练。该项目还提供了一些其他扩散模型的包装器，以及一种改进的采样算法DPM-Solver++，可以在低步数下提高采样质量。</p><p>源: 与必应的对话， 4/2/2023(1) GitHub - Stability-AI/stablediffusion: High-Resolution Image Synthesis .... <a href="https://github.com/Stability-AI/StableDiffusion" target="_blank" rel="noreferrer">https://github.com/Stability-AI/StableDiffusion</a> 访问时间 4/2/2023. (2) Stable Diffusion 2.0 Release — Stability AI. <a href="https://stability.ai/blog/stable-diffusion-v2-release" target="_blank" rel="noreferrer">https://stability.ai/blog/stable-diffusion-v2-release</a> 访问时间 4/2/2023. (3) Stable Diffusion Public Release — Stability AI. <a href="https://stability.ai/blog/stable-diffusion-public-release" target="_blank" rel="noreferrer">https://stability.ai/blog/stable-diffusion-public-release</a> 访问时间 4/2/2023.</p><h2 id="https-api-github-com-repos-automatic1111-stable-diffusion-webui" tabindex="-1"><a href="https://api.github.com/repos/AUTOMATIC1111/stable-diffusion-webui" target="_blank" rel="noreferrer">https://api.github.com/repos/AUTOMATIC1111/stable-diffusion-webui</a> <a class="header-anchor" href="#https-api-github-com-repos-automatic1111-stable-diffusion-webui" aria-hidden="true">#</a></h2><p>这个项目是一个基于稳定扩散模型的图像生成的网页界面，参考了论文《High-Resolution Image Synthesis with Latent Diffusion Models》¹。该项目可以实现多种图像生成的功能，例如文本引导的图像生成、图像外延、图像补全、图像上色等。该项目使用了OpenCLIP作为文本编码器，并在多个图像数据集上进行了训练。该项目还提供了一些其他扩散模型的包装器，以及一种改进的采样算法DPM-Solver++，可以在低步数下提高采样质量。该项目还提供了一个API接口，可以方便地调用模型进行图像生成。</p><p>源: 与必应的对话， 4/2/2023(1) AUTOMATIC1111/stable-diffusion-webui: Stable Diffusion web UI - GitHub. <a href="https://github.com/AUTOMATIC1111/stable-diffusion-webui" target="_blank" rel="noreferrer">https://github.com/AUTOMATIC1111/stable-diffusion-webui</a> 访问时间 4/2/2023. (2) API · AUTOMATIC1111/stable-diffusion-webui Wiki · GitHub. <a href="https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/API" target="_blank" rel="noreferrer">https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/API</a> 访问时间 4/2/2023. (3) 支持国内快速更新 stable-diffusion-webui 的本地安装方法 - 哔哩哔哩. <a href="https://www.bilibili.com/read/cv19359343/" target="_blank" rel="noreferrer">https://www.bilibili.com/read/cv19359343/</a> 访问时间 4/2/2023.</p><h3 id="这个项目有什么应用场景" tabindex="-1">这个项目有什么应用场景？ <a class="header-anchor" href="#这个项目有什么应用场景" aria-hidden="true">#</a></h3><p>这个项目可以用于多种图像生成的应用场景，例如根据文本描述生成图像，根据图像内容进行扩展或补全，根据灰度图像进行上色等。这些应用场景可以为艺术创作、娱乐消遣、图像编辑等领域提供有用的工具。</p><h2 id="https-github-com-tencentarc-gfpgan-git" tabindex="-1"><a href="https://github.com/TencentARC/GFPGAN.git" target="_blank" rel="noreferrer">https://github.com/TencentARC/GFPGAN.git</a> <a class="header-anchor" href="#https-github-com-tencentarc-gfpgan-git" aria-hidden="true">#</a></h2><p>这个项目是一个基于PyTorch的实现，用于生成基于扩散的人脸修复算法，参考了论文《GFP-GAN: Towards Real-World Blind Face Restoration with Generative Facial Prior》¹。该项目利用预训练的人脸GAN（例如StyleGAN2）中包含的丰富和多样的先验知识，来进行无监督的人脸修复。该项目支持多种数据集类型、多GPU和多节点训练、渐进式增长、CLIP引导采样等特性。该项目还提供了一些其他扩散模型的包装器，以及一种改进的采样算法DPM-Solver++，可以在低步数下提高采样质量。</p><p>源: 与必应的对话， 4/2/2023(1) GitHub - TencentARC/GFPGAN: GFPGAN aims at developing Practical .... <a href="https://github.com/TencentARC/GFPGAN" target="_blank" rel="noreferrer">https://github.com/TencentARC/GFPGAN</a> 访问时间 4/2/2023. (2) Releases · TencentARC/GFPGAN · GitHub. <a href="https://github.com/TencentARC/GFPGAN/releases" target="_blank" rel="noreferrer">https://github.com/TencentARC/GFPGAN/releases</a> 访问时间 4/2/2023. (3) 本地部署AI作画工具stable-diffusion-webui(Novel AI)报错排查 - 哔哩哔哩. <a href="https://www.bilibili.com/read/cv20716170" target="_blank" rel="noreferrer">https://www.bilibili.com/read/cv20716170</a> 访问时间 4/2/2023.</p><h2 id="https-github-com-openai-clip-git" tabindex="-1"><a href="https://github.com/openai/CLIP.git" target="_blank" rel="noreferrer">https://github.com/openai/CLIP.git</a> <a class="header-anchor" href="#https-github-com-openai-clip-git" aria-hidden="true">#</a></h2><p>这个项目是一个基于PyTorch的实现，用于生成基于对比的语言-图像预训练（CLIP）的神经网络，参考了论文《Learning Transferable Visual Models From Natural Language Supervision》¹。CLIP可以利用互联网上丰富的自然语言监督，从各种各样的（图像，文本）对中高效地学习视觉概念。CLIP可以通过简单地提供要识别的视觉类别的名称，来应用于任何视觉分类任务，类似于GPT-2和GPT-3的“零样本”能力²³。该项目发现CLIP在没有使用任何原始标注样本的情况下，就能在ImageNet“零样本”上匹配原始ResNet-50的性能，同时在其他更具挑战性的数据集上表现更加鲁棒。</p><p>源: 与必应的对话， 4/2/2023(1) GitHub - openai/CLIP: CLIP (Contrastive Language-Image Pretraining .... <a href="https://github.com/openai/CLIP" target="_blank" rel="noreferrer">https://github.com/openai/CLIP</a> 访问时间 4/2/2023. (2) CLIP: Connecting text and images - OpenAI. <a href="https://openai.com/research/clip/" target="_blank" rel="noreferrer">https://openai.com/research/clip/</a> 访问时间 4/2/2023. (3) mirrors / openai / CLIP · GitCode. <a href="https://gitcode.net/mirrors/openai/CLIP" target="_blank" rel="noreferrer">https://gitcode.net/mirrors/openai/CLIP</a> 访问时间 4/2/2023.</p><h2 id="https-github-com-mlfoundations-open-clip" tabindex="-1"><a href="https://github.com/mlfoundations/open_clip" target="_blank" rel="noreferrer">https://github.com/mlfoundations/open_clip</a> <a class="header-anchor" href="#https-github-com-mlfoundations-open-clip" aria-hidden="true">#</a></h2><p>这个项目是一个开源的CLIP的实现，参考了论文《Learning Transferable Visual Models From Natural Language Supervision》¹。CLIP是一个利用对比学习在大量的（图像，文本）对上进行预训练的神经网络，可以通过自然语言指令来预测给定图像最相关的文本片段，而不需要直接针对任务进行优化，类似于GPT-2和GPT-3的“零样本”能力²³。该项目的目标是使得模型能够使用对比的图像-文本监督进行训练，并研究它们的性质，例如对分布偏移的鲁棒性。该项目的起点是一个能够在相同数据集上匹配原始CLIP模型准确率的实现。具体来说，一个在OpenAI的1500万张YFCC子集上训练的ResNet-50模型，在没有使用任何原始标注样本的情况下，在ImageNet上达到了32.7%的零样本准确率。OpenAI的CLIP模型在相同的YFCC子集上达到了31.3%。为了方便实验，该项目还提供了在概念字幕数据集上训练的代码，该数据集包含300万张图像，其中一个ResNet-50x4模型在ImageNet上达到了22.2%的零样本准确率。此外，该项目还在一个与OpenAI相当大小的数据集LAION-400M上进行了复现研究，并使用了更大的LAION-2B超集。另外，该项目还在一篇论文中研究了对比语言-图像学习的可扩展性规律。该项目已经训练了以下ViT CLIP模型：</p><ul><li>在LAION-400M上训练的ViT-B/32，在ImageNet-1k上零样本准确率为62.9%，与OpenAI的63.2%相当</li><li>在LAION-2B上训练的ViT-B/32，零样本准确率为66.6%</li><li>在LAION-400M上训练的ViT-B/16，零样本准确率为67.1%，低于OpenAI的68.3%</li><li>在LAION-400M上训练的ViT-B/16+ 240x240（比B/16 224x224多约50%的FLOPS），零样本准确率为69.2%</li><li>在LAION-2B上训练的ViT-B/16，零样本准确率为70.2%</li><li>在LAION-400M上训练的ViT-L/14，零样本准确率为72.77%，低于OpenAI的75.5%</li><li>在LAION-2B上训练的ViT-L/14，零样本准确率为75.3%，与OpenAI的75.5%相当</li></ul><p>源: 与必应的对话， 4/2/2023(1) mlfoundations/open_clip: An open source implementation of CLIP. - GitHub. <a href="https://github.com/mlfoundations/open_clip" target="_blank" rel="noreferrer">https://github.com/mlfoundations/open_clip</a> 访问时间 4/2/2023. (2) Issues · mlfoundations/open_clip · GitHub. <a href="https://github.com/mlfoundations/open_clip/issues" target="_blank" rel="noreferrer">https://github.com/mlfoundations/open_clip/issues</a> 访问时间 4/2/2023. (3) 新版SD WebUI卡安装Open_Clip解决方法 - 哔哩哔哩. <a href="https://www.bilibili.com/read/cv21253533" target="_blank" rel="noreferrer">https://www.bilibili.com/read/cv21253533</a> 访问时间 4/2/2023.</p><h2 id="https-github-com-compvis-taming-transformers" tabindex="-1"><a href="https://github.com/CompVis/taming-transformers" target="_blank" rel="noreferrer">https://github.com/CompVis/taming-transformers</a> <a class="header-anchor" href="#https-github-com-compvis-taming-transformers" aria-hidden="true">#</a></h2><p>这个项目的研究方向是利用变换器（transformers）来生成高分辨率的图像。它的主要思想是结合卷积神经网络（CNNs）的局部感知能力和变换器的长距离交互能力，通过以下两个步骤实现：</p><ol><li>使用卷积VQGAN（vector quantized generative adversarial network）学习一个包含丰富上下文信息的图像部件（visual parts）的编码字典（codebook）。</li><li>使用自回归变换器（autoregressive transformer）来高效地建模图像部件在高分辨率图像中的组合方式。</li></ol><p>这个项目可以应用于条件生成任务，例如根据类别或分割信息来控制生成的图像。它在ImageNet数据集上达到了自回归模型中的最佳结果，也是第一个使用变换器来生成兆像素级别的复杂场景图像的工作¹²。</p><h2 id="https-github-com-crowsonkb-k-diffusion" tabindex="-1"><a href="https://github.com/crowsonkb/k-diffusion" target="_blank" rel="noreferrer">https://github.com/crowsonkb/k-diffusion</a> <a class="header-anchor" href="#https-github-com-crowsonkb-k-diffusion" aria-hidden="true">#</a></h2><p>这个项目是一个基于PyTorch的实现，用于生成基于扩散的生成模型，参考了Karras等人（2022）的论文《阐明基于扩散的生成模型的设计空间》¹。该项目还实现了一种通过拼接提高扩散模型效率的方法²。该项目支持多种数据集类型、多GPU和多节点训练、渐进式增长、CLIP引导采样等特性。该项目还提供了一些其他扩散模型的包装器，以及一种改进的采样算法DPM-Solver++，可以在低步数下提高采样质量。</p><p>源: 与必应的对话， 4/2/2023(1) GitHub - crowsonkb/k-diffusion: Karras et al. (2022) diffusion models .... <a href="https://github.com/crowsonkb/k-diffusion" target="_blank" rel="noreferrer">https://github.com/crowsonkb/k-diffusion</a> 访问时间 4/2/2023. (2) crowsonkb (Katherine Crowson) · GitHub. <a href="https://github.com/crowsonkb" target="_blank" rel="noreferrer">https://github.com/crowsonkb</a> 访问时间 4/2/2023. (3) k-diffusion: 从 <a href="https://github.com/crowsonkb/k-diffusion.git" target="_blank" rel="noreferrer">https://github.com/crowsonkb/k-diffusion.git</a> 克隆下来的. <a href="https://gitee.com/fkaijun/k-diffusion" target="_blank" rel="noreferrer">https://gitee.com/fkaijun/k-diffusion</a> 访问时间 4/2/2023.</p><h2 id="https-github-com-sczhou-codeformer" tabindex="-1"><a href="https://github.com/sczhou/CodeFormer" target="_blank" rel="noreferrer">https://github.com/sczhou/CodeFormer</a> <a class="header-anchor" href="#https-github-com-sczhou-codeformer" aria-hidden="true">#</a></h2><p>这个项目是一个基于代码本查找变换器（Codebook Lookup Transformer）的盲人脸修复算法，参考了论文《Towards Robust Blind Face Restoration with Codebook Lookup Transformer》¹。该项目可以对老照片或者AI生成的人脸进行高质量的修复，包括去噪、超分辨率、补全、上色等功能。该项目使用了OpenCLIP作为文本编码器，并在多个人脸数据集上进行了训练。该项目还提供了一些其他扩散模型的包装器，以及一种改进的采样算法DPM-Solver++，可以在低步数下提高采样质量。</p><p>源: 与必应的对话， 4/2/2023(1) GitHub - sczhou/CodeFormer: [NeurIPS 2022] Towards Robust Blind Face .... <a href="https://github.com/sczhou/CodeFormer" target="_blank" rel="noreferrer">https://github.com/sczhou/CodeFormer</a> 访问时间 4/2/2023. (2) Releases · sczhou/CodeFormer · GitHub. <a href="https://github.com/sczhou/CodeFormer/releases" target="_blank" rel="noreferrer">https://github.com/sczhou/CodeFormer/releases</a> 访问时间 4/2/2023. (3) sczhou/codeformer – Run with an API on Replicate. <a href="https://replicate.com/sczhou/codeformer" target="_blank" rel="noreferrer">https://replicate.com/sczhou/codeformer</a> 访问时间 4/2/2023.</p><h3 id="这个项目有什么应用场景-1" tabindex="-1">这个项目有什么应用场景？ <a class="header-anchor" href="#这个项目有什么应用场景-1" aria-hidden="true">#</a></h3><p>这个项目可以用于恢复老照片或者AI生成的人脸的质量，例如去除噪点、增加分辨率、填补缺失部分、增加颜色等。这个项目也可以用于创造新的人脸图像，例如根据文本描述生成人脸，或者根据深度信息生成不同角度的人脸。这个项目可以为历史研究、艺术创作、娱乐消遣等提供有用的工具。</p><h2 id="https-github-com-salesforce-blip" tabindex="-1"><a href="https://github.com/salesforce/BLIP" target="_blank" rel="noreferrer">https://github.com/salesforce/BLIP</a> <a class="header-anchor" href="#https-github-com-salesforce-blip" aria-hidden="true">#</a></h2><p>这个项目是一个基于代码本查找变换器（Codebook Lookup Transformer）的视觉语言预训练框架，参考了论文《BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation》¹。该项目可以实现统一的视觉语言理解和生成，包括图像文本检索、图像描述、视觉问答、视觉推理、视觉对话、零样本视频文本检索和零样本视频问答等任务。该项目使用了OpenCLIP作为文本编码器，并在多个视觉语言数据集上进行了训练。该项目还提出了一种数据集自举方法，用于从噪声较大的网络图像文本对中学习。</p><p>源: 与必应的对话， 4/2/2023(1) GitHub - salesforce/BLIP: PyTorch code for BLIP: Bootstrapping Language .... <a href="https://github.com/salesforce/BLIP" target="_blank" rel="noreferrer">https://github.com/salesforce/BLIP</a> 访问时间 4/2/2023. (2) BLIP: Bootstrapping Language-Image Pre-training for Unified Vision .... <a href="https://blog.salesforceairesearch.com/blip-bootstrapping-language-image-pretraining/" target="_blank" rel="noreferrer">https://blog.salesforceairesearch.com/blip-bootstrapping-language-image-pretraining/</a> 访问时间 4/2/2023. (3) 「理解和生成」的大一统！华人一作提出BLIP模型，「视觉+语言」任务多项SOTA - 知乎. <a href="https://zhuanlan.zhihu.com/p/465788919" target="_blank" rel="noreferrer">https://zhuanlan.zhihu.com/p/465788919</a> 访问时间 4/2/2023.</p><h3 id="这个项目有什么应用场景-2" tabindex="-1">这个项目有什么应用场景？ <a class="header-anchor" href="#这个项目有什么应用场景-2" aria-hidden="true">#</a></h3><p>这个项目可以用于多种视觉语言的应用场景，例如根据文本描述检索或生成图像，根据图像内容生成或回答问题，根据图像和对话上下文进行交互，根据文本描述检索或回答视频等。这些应用场景可以为教育、娱乐、搜索、社交等领域提供有用的功能。</p><h2 id="根据文章-https-github-com-automatic1111-stable-diffusion-webui-wiki-xformers-和其他搜索-详细描述下-xformers-的主要功能" tabindex="-1">根据文章 <a href="https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Xformers" target="_blank" rel="noreferrer">https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Xformers</a> 和其他搜索，详细描述下 xformers 的主要功能 <a class="header-anchor" href="#根据文章-https-github-com-automatic1111-stable-diffusion-webui-wiki-xformers-和其他搜索-详细描述下-xformers-的主要功能" aria-hidden="true">#</a></h2><p>根据文章³和其他搜索，xformers 的主要功能是：</p><ul><li>提供可定制的构建块：独立的、可定制的构建块，可以在没有样板代码的情况下使用。这些组件是领域无关的，xformers 被视觉、自然语言处理等领域的研究人员使用¹。</li><li>以研究为先：xformers 包含了一些尖端的组件，这些组件还没有在主流的库如 pytorch 中提供¹。</li><li>注重效率：因为迭代速度很重要，所以组件要尽可能地快速和节省内存。xformers 包含了自己的 CUDA 内核，但在相关的时候也会调用其他库¹。</li><li>支持加速图像生成和降低显存占用，但会产生非确定性的结果³。</li></ul><p>源: 与必应的对话， 4/2/2023(1) Xformers · AUTOMATIC1111/stable-diffusion-webui Wiki · GitHub. <a href="https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Xformers" target="_blank" rel="noreferrer">https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Xformers</a> 访问时间 4/2/2023. (2) GitHub - facebookresearch/xformers: Hackable and optimized Transformers .... <a href="https://github.com/facebookresearch/xformers" target="_blank" rel="noreferrer">https://github.com/facebookresearch/xformers</a> 访问时间 4/2/2023. (3) Installing xFormers. <a href="https://huggingface.co/docs/diffusers/optimization/xformers" target="_blank" rel="noreferrer">https://huggingface.co/docs/diffusers/optimization/xformers</a> 访问时间 4/2/2023. (4) xformers · PyPI. <a href="https://pypi.org/project/xformers/" target="_blank" rel="noreferrer">https://pypi.org/project/xformers/</a> 访问时间 4/2/2023.</p><h2 id="codeformer-vs-gfpgan" tabindex="-1">CodeFormer vs GFPGAN <a class="header-anchor" href="#codeformer-vs-gfpgan" aria-hidden="true">#</a></h2><p>你好，这是必应。我可以帮你比较这两个项目。根据网上的信息，CodeFormer 和 GFPGAN 都是用于人脸修复和增强的深度学习方法，但它们有一些不同之处。</p><p>CodeFormer 是一个基于编码器-解码器的模型，它可以从低质量的人脸图像中提取特征，并用一个可微分的编码器将它们映射到一个隐空间，然后用一个解码器将它们重建为高质量的人脸图像⁴。CodeFormer 的优点是它可以处理各种噪声和失真，如模糊、压缩、遮挡等，并且可以保留一些细节和纹理³。</p><p>GFPGAN 是一个基于生成对抗网络（GAN）的模型，它可以从低分辨率的人脸图像中生成高分辨率的人脸图像，同时修复一些缺陷和不对称¹。GFPGAN 的优点是它可以重建人脸的结构和形状，并且可以改善眼睛和嘴巴的外观¹²。</p><p>有些人认为，将 CodeFormer 和 GFPGAN 结合起来使用，可以得到比单独使用任何一个更好的结果，因为它们可以互补彼此的优势¹²。例如，GFPGAN 可以修复人脸的轮廓和眼睛，而 CodeFormer 可以增加人脸的纹理和自然感¹²。在 Google Colab 上，有一些界面可以让你同时使用 CodeFormer 和 GFPGAN 来处理你的图像¹²³。</p><p>源: 与必应的对话， 4/2/2023(1) GitHub - sczhou/CodeFormer: [NeurIPS 2022] Towards Robust Blind Face .... <a href="https://github.com/sczhou/CodeFormer" target="_blank" rel="noreferrer">https://github.com/sczhou/CodeFormer</a> 访问时间 4/2/2023. (2) Best Face Restoration &amp; Symmetry Fix Tool for Midjourney/Stable .... <a href="https://www.youtube.com/watch?v=KsYS7NLpK4w" target="_blank" rel="noreferrer">https://www.youtube.com/watch?v=KsYS7NLpK4w</a> 访问时间 4/2/2023. (3) GFPGAN and CodeFormer - seperately and together. : r/StableDiffusion. <a href="https://www.reddit.com/r/StableDiffusion/comments/xjvh05/gfpgan_and_codeformer_seperately_and_together/" target="_blank" rel="noreferrer">https://www.reddit.com/r/StableDiffusion/comments/xjvh05/gfpgan_and_codeformer_seperately_and_together/</a> 访问时间 4/2/2023. (4) Which is better GFPGAN or CodeFormer? : r/StableDiffusion. <a href="https://www.reddit.com/r/StableDiffusion/comments/ybft1g/which_is_better_gfpgan_or_codeformer/" target="_blank" rel="noreferrer">https://www.reddit.com/r/StableDiffusion/comments/ybft1g/which_is_better_gfpgan_or_codeformer/</a> 访问时间 4/2/2023.</p><h3 id="请问-codeformer-和-gfpgan-的性能如何" tabindex="-1">请问 CodeFormer 和 GFPGAN 的性能如何？ <a class="header-anchor" href="#请问-codeformer-和-gfpgan-的性能如何" aria-hidden="true">#</a></h3><p>CodeFormer 和 GFPGAN 的性能取决于输入图像的质量和损坏程度，以及你对输出图像的期望和要求。一般来说，GFPGAN 在提高分辨率和修复人脸结构方面更优秀，而 CodeFormer 在保留细节和纹理方面更优秀¹²。因此，它们的性能也会受到不同的评价标准的影响，比如质量和保真度²。</p><p>有些人在 Google Colab 上测试了 CodeFormer 和 GFPGAN 的性能，并分享了他们的结果和观点¹²。你可以参考他们的例子，或者自己尝试使用这些工具来处理你感兴趣的图像¹²³。这样你可以更直观地比较 CodeFormer 和 GFPGAN 的性能，并找到适合你的设置和组合¹²。</p><p>源: 与必应的对话， 4/2/2023(1) GFPGAN and CodeFormer - seperately and together. : r/StableDiffusion. <a href="https://www.reddit.com/r/StableDiffusion/comments/xjvh05/gfpgan_and_codeformer_seperately_and_together/" target="_blank" rel="noreferrer">https://www.reddit.com/r/StableDiffusion/comments/xjvh05/gfpgan_and_codeformer_seperately_and_together/</a> 访问时间 4/2/2023. (2) CodeFormer - Shangchen Zhou. <a href="https://shangchenzhou.com/projects/CodeFormer/" target="_blank" rel="noreferrer">https://shangchenzhou.com/projects/CodeFormer/</a> 访问时间 4/2/2023. (3) Which is better GFPGAN or CodeFormer? : r/StableDiffusion. <a href="https://www.reddit.com/r/StableDiffusion/comments/ybft1g/which_is_better_gfpgan_or_codeformer/" target="_blank" rel="noreferrer">https://www.reddit.com/r/StableDiffusion/comments/ybft1g/which_is_better_gfpgan_or_codeformer/</a> 访问时间 4/2/2023.</p>',52),s=[i];function n(h,f,l,c,p,b){return t(),r("div",null,s)}const m=e(o,[["render",n]]);export{d as __pageData,m as default};
