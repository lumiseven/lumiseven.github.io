import{_ as a,c as l,o as s,a as n}from"./app.bd26c8cd.js";const D=JSON.parse('{"title":"Ollama","description":"","frontmatter":{},"headers":[],"relativePath":"doc/ai/ollama.md","lastUpdated":1721190099000}'),e={name:"doc/ai/ollama.md"},p=n(`<h1 id="ollama" tabindex="-1"><a href="https://github.com/ollama/ollama" target="_blank" rel="noreferrer">Ollama</a> <a class="header-anchor" href="#ollama" aria-hidden="true">#</a></h1><h2 id="什么是-ollama" tabindex="-1">什么是 Ollama <a class="header-anchor" href="#什么是-ollama" aria-hidden="true">#</a></h2><p>Ollama 是一个开源项目，它是一个功能强大且用户友好的平台，可用于在本地机器上运行 LLM。它充当了 LLM 技术的复杂性与对可访问和可定制的 AI 体验的渴望之间的桥梁。</p><p>从本质上讲，Ollama 简化了下载、安装和与各种 LLM 交互的过程，使用户无需广泛的技术专业知识或依赖基于云的平台即可探索其功能。</p><hr><h2 id="主要特性和功能" tabindex="-1">主要特性和功能 <a class="header-anchor" href="#主要特性和功能" aria-hidden="true">#</a></h2><p>Ollama 拥有一套全面的特性和功能，旨在增强用户体验并最大限度地发挥本地 LLM 的潜力：</p><h2 id="模型库和管理" tabindex="-1">模型库和管理 <a class="header-anchor" href="#模型库和管理" aria-hidden="true">#</a></h2><p>Ollama 提供对多样化且不断扩展的预训练 LLM 模型库的访问，从多功能通用模型到针对特定领域或任务量身定制的专用模型。下载和管理这些模型是一个无缝且简化的过程，无需浏览复杂的模型格式或依赖关系。</p><h2 id="轻松安装和设置" tabindex="-1">轻松安装和设置 <a class="header-anchor" href="#轻松安装和设置" aria-hidden="true">#</a></h2><p>Ollama 的突出特点之一是其用户友好的安装过程。无论您是 Windows、macOS 还是 Linux 用户，Ollama 都提供针对您的操作系统量身定制的直观安装方法，确保流畅、轻松的安装体验。</p><h2 id="本地-api-和集成" tabindex="-1">本地 API 和集成 <a class="header-anchor" href="#本地-api-和集成" aria-hidden="true">#</a></h2><p>Ollama 公开本地 API，允许开发人员将 LLM 无缝集成到他们的应用程序和工作流程中。此 API 促进了您的应用程序和 LLM 之间的有效通信，使您能够发送提示、接收响应并充分利用这些强大的 AI 模型的潜力。</p><h2 id="定制和微调" tabindex="-1">定制和微调 <a class="header-anchor" href="#定制和微调" aria-hidden="true">#</a></h2><p>Ollama 为用户提供了广泛的自定义选项，使他们能够微调 LLM 参数、调整设置并定制模型的行为以满足他们的特定需求和偏好。这种控制级别可确保最佳性能，并允许对不同的模型配置进行实验和探索。</p><h2 id="硬件加速和优化" tabindex="-1">硬件加速和优化 <a class="header-anchor" href="#硬件加速和优化" aria-hidden="true">#</a></h2><p>认识到 LLM 的计算需求，Ollama 智能地利用可用的硬件资源（包括 GPU 和 CPU）来加速推理并优化性能。这可确保高效利用机器的功能，使您能够轻松运行甚至大规模的 LLM。</p><h2 id="交互式用户界面" tabindex="-1">交互式用户界面 <a class="header-anchor" href="#交互式用户界面" aria-hidden="true">#</a></h2><p>Ollama 为高级用户提供了命令行界面，同时还通过与 Open WebUI 等流行工具的无缝集成提供了用户友好的图形界面。这些界面通过提供直观的基于聊天的交互、可视化模型选择和参数调整功能来增强整体体验。</p><h2 id="离线访问和隐私" tabindex="-1">离线访问和隐私 <a class="header-anchor" href="#离线访问和隐私" aria-hidden="true">#</a></h2><p>使用 Ollama 在本地运行 LLM 的主要优势之一是能够完全离线操作，而无需互联网连接。这不仅可以确保不间断的访问和生产力，还可以通过将数据安全地保存在本地环境中来解决隐私问题。</p><h2 id="社区和生态系统" tabindex="-1">社区和生态系统 <a class="header-anchor" href="#社区和生态系统" aria-hidden="true">#</a></h2><p>Ollama 不仅仅是一个平台；这是一个充满活力的社区驱动项目，旨在促进协作、知识共享和持续创新。Ollama 周围活跃的开源社区为其持续开发、错误修复以及有价值的工具和集成的创建做出了贡献，进一步扩展了其功能和影响力。</p><hr><h2 id="使用-ollama-的好处" tabindex="-1">使用 Ollama 的好处 <a class="header-anchor" href="#使用-ollama-的好处" aria-hidden="true">#</a></h2><p>采用 Ollama 进行 LLM 工作可带来多种好处，满足各种需求和用例：</p><h3 id="成本效益" tabindex="-1">成本效益 <a class="header-anchor" href="#成本效益" aria-hidden="true">#</a></h3><p>与通常需要定期订阅费的基于云的 LLM 服务不同，Ollama 是一个免费的开源平台，无需持续的财务投资。这种成本效益使其成为个人、小型企业和预算有限的组织的有吸引力的选择。</p><h3 id="数据隐私和安全" tabindex="-1">数据隐私和安全 <a class="header-anchor" href="#数据隐私和安全" aria-hidden="true">#</a></h3><p>通过在本地运行 LLM，Ollama 可确保您的数据始终在您的控制范围内，解决通常与基于云的 AI 服务相关的数据隐私和安全问题。这对于处理敏感或机密信息的个人和组织来说尤其重要。</p><h3 id="定制和灵活性" tabindex="-1">定制和灵活性 <a class="header-anchor" href="#定制和灵活性" aria-hidden="true">#</a></h3><p>Ollama 广泛的定制选项和对微调 LLM 的支持使您能够根据您的特定需求和领域定制模型的行为。这种灵活性使您能够优化性能、尝试不同的配置并创建符合您独特要求的定制解决方案。</p><h3 id="离线访问和可靠性" tabindex="-1">离线访问和可靠性 <a class="header-anchor" href="#离线访问和可靠性" aria-hidden="true">#</a></h3><p>Ollama 完全离线运行的能力使其成为互联网连接有限或不可靠场景的可靠选择。此功能对于远程位置、移动应用程序或不间断访问 LLM 至关重要的情况非常有用。</p><h3 id="实验和学习" tabindex="-1">实验和学习 <a class="header-anchor" href="#实验和学习" aria-hidden="true">#</a></h3><p>Ollama 提供了一个强大的实验和学习平台，允许用户探索不同 LLM 的功能和局限性，了解它们的优势和劣势，并培养快速工程和 LLM 交互的技能。这种实践方法有助于更深入地了解 AI 技术，并使用户能够突破可能的界限。</p><h3 id="集成和定制" tabindex="-1">集成和定制 <a class="header-anchor" href="#集成和定制" aria-hidden="true">#</a></h3><p>Ollama 的开源性质和广泛的 API 支持促进了与现有工作流程和应用程序的无缝集成。开发人员可以利用 Ollama 构建定制的 AI 驱动工具、服务和解决方案，以满足他们的特定需求，开启创新和创造力的新领域。</p><h2 id="开始使用-ollama" tabindex="-1">开始使用 Ollama <a class="header-anchor" href="#开始使用-ollama" aria-hidden="true">#</a></h2><h3 id="安装和设置" tabindex="-1">安装和设置 <a class="header-anchor" href="#安装和设置" aria-hidden="true">#</a></h3><p>开始使用 Ollama 是一个简单的过程，旨在满足具有不同技术专长水平的用户的需求。安装过程有据可查，并支持多个平台，无论您选择哪种操作系统，都能确保无缝体验。</p><h3 id="windows-安装" tabindex="-1">Windows 安装 <a class="header-anchor" href="#windows-安装" aria-hidden="true">#</a></h3><p>对于 Windows 用户，Ollama 提供了一个用户友好的安装程序，可简化设置过程。只需按照以下步骤操作：</p><ol><li><p>访问官方 Ollama 网站并导航至“下载”部分。</p></li><li><p>下载最新版本的 Ollama Windows 安装程序。</p></li><li><p>运行下载的安装程序并按照屏幕上的说明完成安装过程。</p></li><li><p>安装后，Ollama 将在您的 Windows 计算机上随时可用。</p></li></ol><h3 id="macos-安装" tabindex="-1">macOS 安装 <a class="header-anchor" href="#macos-安装" aria-hidden="true">#</a></h3><p>如果您是 macOS 用户，Ollama 提供了针对您的平台量身定制的专用安装程序：</p><ol><li><p>访问官方 Ollama 网站并导航至“下载”部分。</p></li><li><p>下载最新版本的 Ollama macOS 安装程序。</p></li><li><p>运行下载的安装程序并按照提示完成安装。</p></li><li><p>安装成功后，您会发现 Ollama 在您的 macOS 系统上可用。</p></li></ol><h3 id="linux-安装" tabindex="-1">Linux 安装 <a class="header-anchor" href="#linux-安装" aria-hidden="true">#</a></h3><p>对于 Linux 爱好者，Ollama 提供了一个方便的单行安装脚本，可简化安装过程：</p><ol><li><p>打开您喜欢的终端仿真器，复制并粘贴以下命令：<code>curl -fsSL https://ollama.com/install.sh | sh</code></p></li><li><p>按 Enter 执行命令，让安装脚本处理其余部分。</p></li><li><p>该脚本将自动下载并在您的 Linux 系统上设置 Ollama，确保满足所有必要的依赖项。</p></li></ol><p>安装过程完成后，您现在可以开始您的 Ollama 之旅并探索本地 LLM 的迷人世界。</p><h2 id="选择和下载-llm-模型" tabindex="-1">选择和下载 LLM 模型 <a class="header-anchor" href="#选择和下载-llm-模型" aria-hidden="true">#</a></h2><p>Ollama 的主要优势之一是其广泛的预训练 LLM 模型库，可满足广泛的应用和领域。选择正确的模型对于实现最佳性能和满足您的特定需求至关重要。</p><h3 id="探索-ollama-模型库" tabindex="-1">探索 Ollama 模型库 <a class="header-anchor" href="#探索-ollama-模型库" aria-hidden="true">#</a></h3><p>Ollama 提供了精选的 LLM 模型集合，每个模型都有其独特的特性和功能。一些流行的模型包括：</p><ul><li><p>Llama 2：一种多功能且功能强大的模型，以其在各种任务中的强大性能而闻名，包括文本生成、翻译和问答。</p></li><li><p>Mistral：一种以其创意写作能力而闻名的模型，擅长生成各种文本格式，如诗歌、剧本和音乐作品。</p></li><li><p>Code Llama：一种专门为编码任务量身定制的模型，可帮助开发人员生成代码、调试和理解复杂的编程概念。</p></li><li><p>LLaVA：一种能够处理文本和图像的多模态模型，为创意和视觉应用开辟了可能性。</p></li></ul><p>花点时间探索 Ollama 模型库并熟悉可用的选项。考虑模型大小、性能和与您的预期用例相符的特定功能等因素。</p><h3 id="下载-llm-模型" tabindex="-1">下载 LLM 模型 <a class="header-anchor" href="#下载-llm-模型" aria-hidden="true">#</a></h3><p>一旦您确定了最适合您需求的模型，在 Ollama 中下载它是一个简单的过程：</p><ol><li><p>在您的机器上启动 Ollama 应用程序。</p></li><li><p>导航到 Ollama 界面中的“模型库”部分。</p></li><li><p>浏览可用的模型并选择您想要下载的模型。</p></li><li><p>单击所选模型旁边的“下载”按钮。</p></li><li><p>Ollama 将启动下载过程，从相应的存储库中获取模型文件。</p></li><li><p>等待下载完成。所需时间可能因模型大小和互联网连接速度而异。</p></li><li><p>成功下载后，该模型将可在 Ollama 中使用。</p></li></ol><p>需要注意的是，某些型号可能有特定的硬件要求，例如最低 RAM 容量或 GPU 的存在。请确保您的机器满足所选型号的推荐规格，以确保最佳性能。</p><h2 id="运行和交互llms" tabindex="-1">运行和交互LLMs <a class="header-anchor" href="#运行和交互llms" aria-hidden="true">#</a></h2><p>安装 Ollama 并下载您想要LLM的模型后，您就可以潜入令人兴奋的本地LLM交互世界了。Ollama 提供了多种途径来与这些强大的 AI 模型互动，以满足不同的用户偏好和要求。</p><h2 id="命令行界面-cli" tabindex="-1">命令行界面 （CLI） <a class="header-anchor" href="#命令行界面-cli" aria-hidden="true">#</a></h2><p>对于喜欢更传统和简化方法的用户，Ollama 提供了一个强大的命令行界面 （CLI），允许您直接从终端或控制台进行LLMs交互。</p><h3 id="启动-cli" tabindex="-1">启动 CLI <a class="header-anchor" href="#启动-cli" aria-hidden="true">#</a></h3><p>要启动 Ollama CLI，请按照下列步骤操作：</p><ol><li><p>打开终端或控制台应用程序。</p></li><li><p>使用相应的命令导航到 Ollama 的安装目录（例如 cd /path/to/ollama ）。</p></li><li><p>键入以下命令： ollama run [model_name]</p></li><li><p>替换 [model_name] 为要运行的LLM模型的名称（例如 ollama run llama2 ）。</p></li><li><p>执行命令后，Ollama CLI 将初始化并加载指定的LLM模型，为交互做好准备。</p></li></ol><h3 id="与llm交互" tabindex="-1">与LLM交互 <a class="header-anchor" href="#与llm交互" aria-hidden="true">#</a></h3><p>加载模型后，您可以通过直接在终端中键入提示或查询来开始与它交互。将LLM处理您的输入并生成响应，该响应将显示在控制台中。</p><p>例如，您可以键入：</p><div class="language-txt"><button title="Copy Code" class="copy"></button><span class="lang">txt</span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#A6ACCD;">Human: What is the capital of France?</span></span>
<span class="line"><span style="color:#A6ACCD;"></span></span></code></pre></div><p>然后，将LLM处理您的查询并提供适当的响应，例如：</p><div class="language-txt"><button title="Copy Code" class="copy"></button><span class="lang">txt</span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#A6ACCD;">AI: The capital of France is Paris.</span></span>
<span class="line"><span style="color:#A6ACCD;"></span></span></code></pre></div><p>您可以继续此对话流程，提出后续问题，提供其他上下文，或使用 LLM.</p><h3 id="cli-命令和选项" tabindex="-1">CLI 命令和选项 <a class="header-anchor" href="#cli-命令和选项" aria-hidden="true">#</a></h3><p>Ollama CLI 提供了一系列命令和选项来增强您的体验并提供对LLM交互的更好控制：</p><ul><li><code>/help</code> 或 <code>/?</code> ：显示可用命令及其描述的列表，帮助您浏览 CLI 的功能。</li><li><code>/temperature [value]</code> ：调整温度参数，控制LLM响应的随机性和创造性。</li><li><code>/top_k [value]</code> ：设置 top-k 采样值，该值确定生成响应时LLM考虑的标记数。</li><li><code>/bye</code> ：停止当前LLM会话并返回到命令提示符。</li></ul><p>这些只是可用命令和选项的几个示例。请参阅 Ollama 文档或使用 CLI 中 <code>/help</code> 的命令获取完整列表和详细说明。</p><h2 id="web-用户界面-ui" tabindex="-1">Web 用户界面 （UI） <a class="header-anchor" href="#web-用户界面-ui" aria-hidden="true">#</a></h2><p>虽然 CLI 提供了一种强大而高效的交互方式LLMs，但某些用户可能更喜欢更直观和直观的体验。Ollama 通过与社区开发的各种基于 Web 的用户界面 （UI） 无缝集成来满足这一需求。</p><h3 id="打开-webui" tabindex="-1">打开 WebUI <a class="header-anchor" href="#打开-webui" aria-hidden="true">#</a></h3><p>Ollama 最流行的 Web UI 之一是 Open WebUI。这个功能丰富的界面为与 LLMs进行交互提供了一个用户友好的环境，并配有类似聊天的界面、模型选择选项和高级参数控制。</p><h4 id="访问-open-webui" tabindex="-1">访问 Open WebUI <a class="header-anchor" href="#访问-open-webui" aria-hidden="true">#</a></h4><ol><li><p>在您的计算机上启动 Ollama 应用程序。</p></li><li><p>导航到 Ollama 界面中的“集成”或“Web UI”部分。</p></li><li><p>从可用选项中选择“打开 WebUI”。</p></li><li><p>Ollama 将在您的默认 Web 浏览器中自动启动 Open WebUI。</p></li></ol><p>加载 Open WebUI 后，您将看到一个干净直观的界面，类似于流行的聊天应用程序。</p><h4 id="与llm交互-1" tabindex="-1">与LLM交互 <a class="header-anchor" href="#与llm交互-1" aria-hidden="true">#</a></h4><p>在 Open WebUI 中，您可以以对话方式与模型LLM互动，在输入字段中键入提示或查询，并实时接收模型的响应。</p><p>该界面还提供其他功能和控件，例如：</p><ul><li>Model Selection: 从 Ollama 安装中的可用LLM型号中进行选择。</li><li>Parameter Adjustment: 修改温度、top-k 和重复惩罚等设置以微调 LLM的行为。</li><li>Context Management: 通过查看以前的消息和响应来维护对话上下文。</li><li>Advanced Options: 访问更高级的功能，如网页浏览、代码执行和图像生成（取决于LLM的功能）。</li></ul><p>Open WebUI 提供了一种视觉上吸引人且用户友好的交互方式LLMs，使其成为喜欢图形界面而不是命令行体验的用户的绝佳选择。</p><h3 id="社区开发的-web-ui" tabindex="-1">社区开发的 Web UI <a class="header-anchor" href="#社区开发的-web-ui" aria-hidden="true">#</a></h3><p>除了 Open WebUI 之外，充满活力的 Ollama 社区还开发了各种其他 Web UI，每个 UI 都提供独特的特性和功能。一些流行的选项包括：</p><ul><li>Hollama：一个基于 Web 的界面，专注于自定义和可扩展性，允许用户创建自己的自定义 UI 组件。</li><li>AnythingLLM：一个桌面应用程序，提供全面的LLM体验，包括用于整合外部知识源的 RAG（检索增强生成）等功能。</li><li>SillyTavern：SillyTavern 专为交互式讲故事和角色扮演而设计，允许用户创建角色、构建世界并参与由 LLMs. 浏览 Ollama 社区论坛、文档和在线资源，以发现和了解这些替代 Web UI，并选择最符合您的偏好和要求的 UI。</li></ul><h2 id="定制和微调llms" tabindex="-1">定制和微调LLMs <a class="header-anchor" href="#定制和微调llms" aria-hidden="true">#</a></h2><p>使用 Ollama 在本地运行LLMs的主要优势之一是能够自定义和微调模型以满足您的特定需求。这种级别的控制和灵活性通常不适用于基于LLM云的服务，这些服务通常为模型配置提供有限的选项。</p><h2 id="提示工程" tabindex="-1">提示工程 <a class="header-anchor" href="#提示工程" aria-hidden="true">#</a></h2><p>提示工程是制作有效提示的艺术，这些提示可以指导生成LLM所需的输出。Ollama 提供了各种工具和技术来帮助您掌握这项技能：</p><h3 id="系统提示" tabindex="-1">系统提示 <a class="header-anchor" href="#系统提示" aria-hidden="true">#</a></h3><p>系统提示是在处理主提示之前提供给系统的LLM说明或指南。这些系统提示可以影响模型的行为、语气和响应风格。</p><p>例如，您可以提供系统提示，例如：</p><div class="language-txt"><button title="Copy Code" class="copy"></button><span class="lang">txt</span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#A6ACCD;">&quot;You are a professional and polite writing assistant. Please respond in a formal and concise manner.&quot;</span></span>
<span class="line"><span style="color:#A6ACCD;"></span></span></code></pre></div><p>此系统提示将指示生成LLM正式、礼貌和简洁的响应，并定制其输出以匹配指定的准则。</p><h3 id="提示模板" tabindex="-1">提示模板 <a class="header-anchor" href="#提示模板" aria-hidden="true">#</a></h3><p>Ollama 允许您创建和保存提示模板，这些模板可以在不同的LLM会话中重复使用和共享。这些模板可以包含动态内容的占位符，从而更轻松地为类似任务生成一致的输出。</p><p>例如，您可以创建一个用于生成产品描述的提示模板：</p><div class="language-txt"><button title="Copy Code" class="copy"></button><span class="lang">txt</span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#A6ACCD;">&quot;Write a compelling product description for [PRODUCT_NAME], highlighting its key features and benefits.&quot;</span></span>
<span class="line"><span style="color:#A6ACCD;"></span></span></code></pre></div><p>通过 [PRODUCT_NAME] 替换为实际的产品名称，您可以快速生成量身定制的产品描述，而无需每次都重新输入整个提示。</p><h3 id="few-shot-learning-小样本学习" tabindex="-1">Few-Shot Learning 小样本学习 <a class="header-anchor" href="#few-shot-learning-小样本学习" aria-hidden="true">#</a></h3><p>小样本学习是一种技术，涉及提供LLM所需输出的几个示例以及相应的提示。这有助于模型更好地理解任务并生成更准确和相关的响应。 例如，如果你想LLM生成俳句（一种日本诗歌形式），你可以提供一些写得很好的俳句的例子，以及它们的提示。然后，LLM将从这些示例中学习，并更好地根据您的提示生成新的俳句。 Ollama 的提示工程工具和技术使您能够塑造 LLM的行为和输出，确保它们符合您的特定要求和偏好。</p><h2 id="微调llms" tabindex="-1">微调LLMs <a class="header-anchor" href="#微调llms" aria-hidden="true">#</a></h2><p>虽然提示工程允许您指导 LLM的响应，但微调通过修改模型的参数和权重来优化其针对特定任务或域的性能，从而进一步实现自定义。</p><h3 id="微调过程" tabindex="-1">微调过程 <a class="header-anchor" href="#微调过程" aria-hidden="true">#</a></h3><ol><li>数据准备：收集与目标任务或域相关的数据集。该数据集应包含所需输入输出对的示例。</li><li>模型选择：从 Ollama 库中选择与您的任务和硬件功能相符的适当基础LLM模型。</li><li>微调配置：根据数据集和硬件约束，设置微调参数，如学习率、批量大小、纪元数等。</li><li>训练：启动微调过程，包括使用准备好的数据集更新模型的参数和权重。</li><li>评估：在单独的评估数据集上评估微调模型的性能，以确保其满足您的要求。</li><li>部署：一旦对微调模型的性能感到满意，就可以将其部署在 Ollama 中，以便在您的应用程序或工作流中使用。</li></ol><p>微调可以显著提高 LLM的准确性和对特定任务的相关性，使其成为需要特定领域或专用语言模型的应用程序的宝贵工具。</p><h2 id="ollama的集成生态系统" tabindex="-1">Ollama的集成生态系统 <a class="header-anchor" href="#ollama的集成生态系统" aria-hidden="true">#</a></h2><p>虽然 Ollama 作为本地运行LLMs的独立平台表现出色，但它的真正力量在于它能够与各种工具和框架集成，使开发人员能够构建复杂的 AI 驱动的应用程序和解决方案。</p><h3 id="python-集成" tabindex="-1">Python 集成 <a class="header-anchor" href="#python-集成" aria-hidden="true">#</a></h3><p>Python 是一种流行的编程语言，广泛应用于数据科学和机器学习领域。Ollama 提供与 Python 的无缝集成，允许开发人员在其 Python 项目和工作流程中利用 Python LLMs 的强大功能。</p><h3 id="ollama-python-库" tabindex="-1">Ollama Python 库 <a class="header-anchor" href="#ollama-python-库" aria-hidden="true">#</a></h3><p>官方的 Ollama Python 库简化了在 Python 代码中进行LLMs交互的过程。只需几行代码，开发人员就可以：</p><ul><li>加载并运行 LLM Ollama 库中提供的模型。</li><li>发送提示并接收来自 LLM的生成响应。</li><li>动态调整模型参数和配置。</li><li>集成LLMs到更大的 Python 应用程序和管道中。</li></ul><p>Python 代码示例：</p><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#89DDFF;font-style:italic;">from</span><span style="color:#A6ACCD;"> ollama </span><span style="color:#89DDFF;font-style:italic;">import</span><span style="color:#A6ACCD;"> LLM</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;"># Load the Llama 2 model</span></span>
<span class="line"><span style="color:#A6ACCD;">model </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">LLM</span><span style="color:#89DDFF;">(</span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">llama2</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;"># Generate text based on a prompt</span></span>
<span class="line"><span style="color:#A6ACCD;">prompt </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">Write a short story about a curious robot exploring a new planet.</span><span style="color:#89DDFF;">&quot;</span></span>
<span class="line"><span style="color:#A6ACCD;">output </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> model</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">generate</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">prompt</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#82AAFF;">print</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">output</span><span style="color:#89DDFF;">)</span></span>
<span class="line"></span></code></pre></div><p>这个简单的示例演示了使用 Ollama Python 库加载LLM模型并根据给定提示生成文本是多么容易。</p><h3 id="langchain集成" tabindex="-1">LangChain集成 <a class="header-anchor" href="#langchain集成" aria-hidden="true">#</a></h3><p>LangChain是一个流行的开源框架，用于构建具有大型语言模型的应用程序。Ollama 与 LangChain 无缝集成，使开发人员能够在本地运行LLMs时利用其强大的功能和工具。</p><p>通过LangChain集成，开发人员可以：</p><ul><li>构建检索增强生成 （RAG） 系统，将输出与来自外部数据源的信息相结合LLM。</li><li>创建代理和内存组件以维护对话上下文和状态。</li><li>利用LangChain广泛的工具和实用程序进行快速的工程设计、评估和模型管理。</li><li>通过与其他组件（如知识库、数据库和 API）结合LLMs使用，开发复杂的 AI 应用程序和工作流。</li></ul><p>LangChain代码示例：</p><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#89DDFF;font-style:italic;">from</span><span style="color:#A6ACCD;"> langchain </span><span style="color:#89DDFF;font-style:italic;">import</span><span style="color:#A6ACCD;"> LLMChain</span><span style="color:#89DDFF;">,</span><span style="color:#A6ACCD;"> PromptTemplate</span></span>
<span class="line"><span style="color:#89DDFF;font-style:italic;">from</span><span style="color:#A6ACCD;"> ollama </span><span style="color:#89DDFF;font-style:italic;">import</span><span style="color:#A6ACCD;"> LLM</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;"># Load the Llama 2 model</span></span>
<span class="line"><span style="color:#A6ACCD;">llm </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">LLM</span><span style="color:#89DDFF;">(</span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">llama2</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;"># Define a prompt template</span></span>
<span class="line"><span style="color:#A6ACCD;">template </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">&quot;&quot;&quot;</span></span>
<span class="line"><span style="color:#C3E88D;">You are a helpful AI assistant.</span></span>
<span class="line"><span style="color:#C3E88D;">Human: </span><span style="color:#F78C6C;">{human_input}</span></span>
<span class="line"><span style="color:#C3E88D;">Assistant:</span><span style="color:#89DDFF;">&quot;&quot;&quot;</span></span>
<span class="line"><span style="color:#A6ACCD;">prompt </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">PromptTemplate</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">template</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#A6ACCD;font-style:italic;">input_variables</span><span style="color:#89DDFF;">=[</span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">human_input</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">])</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;"># Create an LLMChain</span></span>
<span class="line"><span style="color:#A6ACCD;">chain </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">LLMChain</span><span style="color:#89DDFF;">(</span><span style="color:#A6ACCD;font-style:italic;">prompt</span><span style="color:#89DDFF;">=</span><span style="color:#82AAFF;">prompt</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#A6ACCD;font-style:italic;">llm</span><span style="color:#89DDFF;">=</span><span style="color:#82AAFF;">llm</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;"># Generate a response</span></span>
<span class="line"><span style="color:#A6ACCD;">output </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> chain</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">run</span><span style="color:#89DDFF;">(</span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">What is the capital of France?</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#82AAFF;">print</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">output</span><span style="color:#89DDFF;">)</span></span>
<span class="line"></span></code></pre></div><p>此示例演示了如何将 LangChain 与 Ollama 一起使用来创建 LLMChain 并根据用户提示生成响应。</p><h3 id="llamaindex-集成" tabindex="-1">LlamaIndex 集成 <a class="header-anchor" href="#llamaindex-集成" aria-hidden="true">#</a></h3><p>LlamaIndex 是另一个功能强大的开源项目，它通过提供数据索引和检索LLMs工具来补充 Ollama 。这种集成使开发人员能够构建检索增强生成 （RAG） 系统，将输出与来自外部数据源的信息相结合LLM。</p><p>通过 LlamaIndex 集成，开发人员可以：</p><ul><li>索引和存储大型数据集、文档或知识库。</li><li>根据用户提示或查询从索引数据中检索相关信息。</li><li>将检索到的信息与LLM输出相结合，以生成更明智和上下文感知的响应。</li><li>构建利用 LlamaIndex 的生成功能LLMs和检索功能的应用程序。</li></ul><p>示例 LlamaIndex 代码：</p><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#89DDFF;font-style:italic;">from</span><span style="color:#A6ACCD;"> llama_index </span><span style="color:#89DDFF;font-style:italic;">import</span><span style="color:#A6ACCD;"> GPTVectorStoreIndex</span><span style="color:#89DDFF;">,</span><span style="color:#A6ACCD;"> SimpleDirectoryReader</span></span>
<span class="line"><span style="color:#89DDFF;font-style:italic;">from</span><span style="color:#A6ACCD;"> ollama </span><span style="color:#89DDFF;font-style:italic;">import</span><span style="color:#A6ACCD;"> LLM</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;"># Load the Llama 2 model</span></span>
<span class="line"><span style="color:#A6ACCD;">llm </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">LLM</span><span style="color:#89DDFF;">(</span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">llama2</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;"># Create a document reader</span></span>
<span class="line"><span style="color:#A6ACCD;">documents </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">SimpleDirectoryReader</span><span style="color:#89DDFF;">(</span><span style="color:#89DDFF;">&#39;</span><span style="color:#C3E88D;">path/to/documents</span><span style="color:#89DDFF;">&#39;</span><span style="color:#89DDFF;">).</span><span style="color:#82AAFF;">load_data</span><span style="color:#89DDFF;">()</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;"># Create a vector store index</span></span>
<span class="line"><span style="color:#A6ACCD;">index </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> GPTVectorStoreIndex</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">from_documents</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">documents</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> llm</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;"># Query the index</span></span>
<span class="line"><span style="color:#A6ACCD;">query </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">What is the capital of France?</span><span style="color:#89DDFF;">&quot;</span></span>
<span class="line"><span style="color:#A6ACCD;">response </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> index</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">query</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">query</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#82AAFF;">print</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">response</span><span style="color:#89DDFF;">)</span></span>
<span class="line"></span></code></pre></div><p>在此示例中，LlamaIndex 用于从文档目录创建矢量存储索引。然后可以使用 Ollama LLM查询索引，将检索到的信息与LLM生成功能相结合，以提供更明智的响应。 这些与 Python、LangChain 和 LlamaIndex 的集成只是一个开始。Ollama 及其活跃社区的开源性质促进了与各种其他工具和框架的持续开发和集成，进一步扩展了其功能，使开发人员能够构建创新的 AI 驱动的应用程序。</p><p><a href="https://medium.com/@1kg/ollama-what-is-ollama-9f73f3eafa8b#:~:text=Ollama%20is%20an%20open%2Dsource%20project%20that%20serves%20as%20a,accessible%20and%20customizable%20AI%20experience." target="_blank" rel="noreferrer">访问原文</a></p><hr><p>[Note] linux 系统中使用 ollama 定义 model 位置 端口绑定时 使用 <code>systemd</code> <code>Environment</code></p><div class="language-txt"><button title="Copy Code" class="copy"></button><span class="lang">txt</span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#A6ACCD;">[Service]</span></span>
<span class="line"><span style="color:#A6ACCD;">...</span></span>
<span class="line"><span style="color:#A6ACCD;">Environment=&quot;OLLAMA_MODELS=/mnt/data1/D2/ollama/models/&quot; # 模型路径</span></span>
<span class="line"><span style="color:#A6ACCD;">Environment=&quot;OLLAMA_HOST=0.0.0.0&quot; # 绑定到 0.0.0.0</span></span>
<span class="line"><span style="color:#A6ACCD;">...</span></span>
<span class="line"><span style="color:#A6ACCD;"></span></span></code></pre></div>`,143),o=[p];function t(i,r,c,h,d,y){return s(),l("div",null,o)}const L=a(e,[["render",t]]);export{D as __pageData,L as default};
