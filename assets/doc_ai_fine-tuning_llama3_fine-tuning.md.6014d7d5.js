import{_ as s,c as n,o as a,b as l}from"./app.36a7f0d2.js";const C=JSON.parse('{"title":"Llama3 fine-tuning","description":"","frontmatter":{},"headers":[],"relativePath":"doc/ai/fine-tuning/llama3_fine-tuning.md","lastUpdated":1720314909000}'),o={name:"doc/ai/fine-tuning/llama3_fine-tuning.md"},p=l(`<h1 id="llama3-fine-tuning" tabindex="-1">Llama3 fine-tuning <a class="header-anchor" href="#llama3-fine-tuning" aria-hidden="true">#</a></h1><h2 id="为什么选择-llama3-进行微调" tabindex="-1">为什么选择 Llama3 进行微调 <a class="header-anchor" href="#为什么选择-llama3-进行微调" aria-hidden="true">#</a></h2><ol><li><strong>高精度</strong>：Llama3 在各种基准测试中表现出更加先进的准确性，包括 SuperGLUE、GLUE、SQuAD</li><li><strong>高效率</strong>：Llama3 经过优化，可以高效运行，即使是在资源有限的设备上也是如此。这意味着它可以用于各种实时应用程序</li><li><strong>泛化能力强</strong>：Llama3 能够泛化到与训练数据不同的数据，这意味着它可以用于各种新任务和域</li><li><strong>更灵活</strong>：Llama3 可以使用多种方法进行微调，这使其可用于各种任务</li></ol><h1 id="unsloth" tabindex="-1"><a href="https://github.com/unslothai/unsloth" target="_blank" rel="noreferrer">unsloth</a> <a class="header-anchor" href="#unsloth" aria-hidden="true">#</a></h1><p>Unsloth 是一个新兴的人工智能公司，专注于加快和优化大型语言模型（LLMs）的训练过程。传统上，LLMs的训练需要大量的计算资源和内存，耗时较长。为了解决这一问题，Unsloth 开发了一款名为 Unsloth 的软件，能够将训练速度提升高达30倍，并将内存使用减少60%。</p><h2 id="unsloth-主要特点和优势" tabindex="-1">unsloth 主要特点和优势： <a class="header-anchor" href="#unsloth-主要特点和优势" aria-hidden="true">#</a></h2><p>通过 QLoRA 和 LoRA 技术来加速 LLM 的微调，能够提升 2-5 倍的性能并减少 70% 的内存使用量</p><ol><li><p><strong>技术优化</strong>：</p><ul><li><strong>手动自动微分（Manual Autograd）</strong>：通过手动计算梯度来优化模型更新过程，加快训练速度。</li><li><strong>链式矩阵乘法（Chained Matrix Multiplication）</strong>：高效地优化矩阵乘法，是LLMs训练中的关键步骤。</li><li><strong>Triton语言内核（Triton Language Kernels）</strong>：使用由OpenAI开发的高性能计算语言 Triton 重写关键训练代码。</li><li><strong>Flash Attention</strong>：通过 xformers 和 Tri Dao 的实现，帮助模型集中注意力于输入数据的重要部分。</li></ul></li><li><p><strong>工作原理</strong></p><ul><li>通过将 LLM 的权重分解为低秩矩阵来减少内存使用量</li><li>能够在更小的 GPU 上训练 LLM，或者在更大的 GPU 上训练更大的 LLM</li><li>可以有效地加速各种 LLM 的微调，包括 GPT-3 Jurassic-1 Jumbo 和 Megatron-Turing NLG</li></ul></li><li><p><strong>兼容性和可访问性</strong>：</p><ul><li>支持 NVIDIA、Intel 和 AMD 等主流GPU，无需购买昂贵的新硬件即可使用。</li><li>提供免费的开源版本，任何人都可以在GitHub上获取并体验快速训练和更少内存使用的好处。</li></ul></li><li><p><strong>适用场景</strong>：</p><ul><li>NLP</li><li>机器翻译</li><li>文本生成</li><li>问答系统</li><li>聊天机器人</li></ul></li><li><p><strong>支持的语言模型</strong>：</p><ul><li>支持多种流行的语言模型，使用户可以轻松应用优化技术到其喜爱的模型上。</li></ul></li><li><p><strong>性能测试结果</strong>：</p><ul><li>在多个数据集和硬件设置下，Unsloth 显著减少了训练时间和内存使用量，例如在 Alpaca 数据集上，将训练时间从85小时缩短到仅3小时，内存使用从16.7GB降至6.9GB。</li></ul></li><li><p><strong>社区和未来计划</strong>：</p><ul><li>积极与AI社区互动，鼓励用户尝试其开源软件并提供反馈。</li><li>提供Pro和Max版本，支持多GPU和完整的LLM训练功能。</li><li>未来计划包括进一步增加推理速度、实施 sqrt 梯度检查点技术以进一步减少内存使用、优化训练方法等。</li></ul></li><li><p><strong>结论</strong>：</p><ul><li>Unsloth 在加快和优化AI语言模型训练方面取得了显著进展。他们的技术不仅提高了训练效率，还通过兼容性、可访问性和社区参与，致力于在AI和自然语言处理领域产生深远影响。</li></ul></li></ol><p>这些特点使得 Unsloth 成为当前AI领域中备受关注的技术创新之一。</p><h1 id="具体实现" tabindex="-1">具体实现 <a class="header-anchor" href="#具体实现" aria-hidden="true">#</a></h1><p><a href="https://colab.research.google.com/drive/12YnUcuS640v78N75lw_ydmbuJXrLvMYn#scrollTo=iHjt_SMYsd3P" target="_blank" rel="noreferrer">colab</a></p><ol><li>安装 unsloth</li></ol><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#676E95;font-style:italic;"># Installs Unsloth, Xformers (Flash Attention) and all other packages!</span></span>
<span class="line"><span style="color:#A6ACCD;">!pip install </span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git</span><span style="color:#89DDFF;">&quot;</span></span>
<span class="line"><span style="color:#A6ACCD;">!pip install --no</span><span style="color:#89DDFF;">-</span><span style="color:#A6ACCD;">deps xformers </span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">trl&lt;0.9.0</span><span style="color:#89DDFF;">&quot;</span><span style="color:#A6ACCD;"> peft accelerate bitsandbytes</span></span>
<span class="line"></span></code></pre></div><ol start="2"><li>加载模型</li></ol><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#89DDFF;font-style:italic;">from</span><span style="color:#A6ACCD;"> unsloth </span><span style="color:#89DDFF;font-style:italic;">import</span><span style="color:#A6ACCD;"> FastLanguageModel</span></span>
<span class="line"><span style="color:#89DDFF;font-style:italic;">import</span><span style="color:#A6ACCD;"> torch</span></span>
<span class="line"><span style="color:#A6ACCD;">max_seq_length </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> </span><span style="color:#F78C6C;">2048</span><span style="color:#A6ACCD;"> </span><span style="color:#676E95;font-style:italic;"># Choose any! We auto support RoPE Scaling internally!</span></span>
<span class="line"><span style="color:#A6ACCD;">dtype </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">None</span><span style="color:#A6ACCD;"> </span><span style="color:#676E95;font-style:italic;"># None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+</span></span>
<span class="line"><span style="color:#A6ACCD;">load_in_4bit </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">True</span><span style="color:#A6ACCD;"> </span><span style="color:#676E95;font-style:italic;"># Use 4bit quantization to reduce memory usage. Can be False.</span></span>
<span class="line"></span>
<span class="line"><span style="color:#676E95;font-style:italic;"># 4bit pre quantized models we support for 4x faster downloading + no OOMs.</span></span>
<span class="line"><span style="color:#A6ACCD;">fourbit_models </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">[</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">unsloth/mistral-7b-v0.3-bnb-4bit</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">,</span><span style="color:#A6ACCD;">      </span><span style="color:#676E95;font-style:italic;"># New Mistral v3 2x faster!</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">unsloth/mistral-7b-instruct-v0.3-bnb-4bit</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">,</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">unsloth/llama-3-8b-bnb-4bit</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">,</span><span style="color:#A6ACCD;">           </span><span style="color:#676E95;font-style:italic;"># Llama-3 15 trillion tokens model 2x faster!</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">unsloth/llama-3-8b-Instruct-bnb-4bit</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">,</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">unsloth/llama-3-70b-bnb-4bit</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">,</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">unsloth/Phi-3-mini-4k-instruct</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">,</span><span style="color:#A6ACCD;">        </span><span style="color:#676E95;font-style:italic;"># Phi-3 2x faster!</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">unsloth/Phi-3-medium-4k-instruct</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">,</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">unsloth/mistral-7b-bnb-4bit</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">,</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">unsloth/gemma-7b-bnb-4bit</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">,</span><span style="color:#A6ACCD;">             </span><span style="color:#676E95;font-style:italic;"># Gemma 2.2x faster!</span></span>
<span class="line"><span style="color:#89DDFF;">]</span><span style="color:#A6ACCD;"> </span><span style="color:#676E95;font-style:italic;"># More models at https://huggingface.co/unsloth</span></span>
<span class="line"></span>
<span class="line"><span style="color:#A6ACCD;">model</span><span style="color:#89DDFF;">,</span><span style="color:#A6ACCD;"> tokenizer </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> FastLanguageModel</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">from_pretrained</span><span style="color:#89DDFF;">(</span></span>
<span class="line"><span style="color:#82AAFF;">    </span><span style="color:#A6ACCD;font-style:italic;">model_name</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">=</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">unsloth/llama-3-8b-bnb-4bit</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">,</span></span>
<span class="line"><span style="color:#82AAFF;">    </span><span style="color:#A6ACCD;font-style:italic;">max_seq_length</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">=</span><span style="color:#82AAFF;"> max_seq_length</span><span style="color:#89DDFF;">,</span></span>
<span class="line"><span style="color:#82AAFF;">    </span><span style="color:#A6ACCD;font-style:italic;">dtype</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">=</span><span style="color:#82AAFF;"> dtype</span><span style="color:#89DDFF;">,</span></span>
<span class="line"><span style="color:#82AAFF;">    </span><span style="color:#A6ACCD;font-style:italic;">load_in_4bit</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">=</span><span style="color:#82AAFF;"> load_in_4bit</span><span style="color:#89DDFF;">,</span></span>
<span class="line"><span style="color:#82AAFF;">    </span><span style="color:#676E95;font-style:italic;"># token = &quot;hf_...&quot;, # use one if using gated models like meta-llama/Llama-2-7b-hf</span></span>
<span class="line"><span style="color:#89DDFF;">)</span></span>
<span class="line"></span></code></pre></div><ol start="3"><li>修改模型 结合 QLoRA/LoRA 配置优化微调参数</li></ol><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#A6ACCD;">model </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> FastLanguageModel</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">get_peft_model</span><span style="color:#89DDFF;">(</span></span>
<span class="line"><span style="color:#82AAFF;">    model</span><span style="color:#89DDFF;">,</span></span>
<span class="line"><span style="color:#82AAFF;">    </span><span style="color:#A6ACCD;font-style:italic;">r</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">=</span><span style="color:#82AAFF;"> </span><span style="color:#F78C6C;">16</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#676E95;font-style:italic;"># Choose any number &gt; 0 ! Suggested 8, 16, 32, 64, 128</span></span>
<span class="line"><span style="color:#82AAFF;">    </span><span style="color:#A6ACCD;font-style:italic;">target_modules</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">=</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">[</span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">q_proj</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">k_proj</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">v_proj</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">o_proj</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">,</span></span>
<span class="line"><span style="color:#82AAFF;">                      </span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">gate_proj</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">up_proj</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">down_proj</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">,],</span></span>
<span class="line"><span style="color:#82AAFF;">    </span><span style="color:#A6ACCD;font-style:italic;">lora_alpha</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">=</span><span style="color:#82AAFF;"> </span><span style="color:#F78C6C;">16</span><span style="color:#89DDFF;">,</span></span>
<span class="line"><span style="color:#82AAFF;">    </span><span style="color:#A6ACCD;font-style:italic;">lora_dropout</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">=</span><span style="color:#82AAFF;"> </span><span style="color:#F78C6C;">0</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#676E95;font-style:italic;"># Supports any, but = 0 is optimized</span></span>
<span class="line"><span style="color:#82AAFF;">    </span><span style="color:#A6ACCD;font-style:italic;">bias</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">=</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">none</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;">    </span><span style="color:#676E95;font-style:italic;"># Supports any, but = &quot;none&quot; is optimized</span></span>
<span class="line"><span style="color:#82AAFF;">    </span><span style="color:#676E95;font-style:italic;"># [NEW] &quot;unsloth&quot; uses 30% less VRAM, fits 2x larger batch sizes!</span></span>
<span class="line"><span style="color:#82AAFF;">    </span><span style="color:#A6ACCD;font-style:italic;">use_gradient_checkpointing</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">=</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">unsloth</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#676E95;font-style:italic;"># True or &quot;unsloth&quot; for very long context</span></span>
<span class="line"><span style="color:#82AAFF;">    </span><span style="color:#A6ACCD;font-style:italic;">random_state</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">=</span><span style="color:#82AAFF;"> </span><span style="color:#F78C6C;">3407</span><span style="color:#89DDFF;">,</span></span>
<span class="line"><span style="color:#82AAFF;">    </span><span style="color:#A6ACCD;font-style:italic;">use_rslora</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">=</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">False,</span><span style="color:#82AAFF;">  </span><span style="color:#676E95;font-style:italic;"># We support rank stabilized LoRA</span></span>
<span class="line"><span style="color:#82AAFF;">    </span><span style="color:#A6ACCD;font-style:italic;">loftq_config</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">=</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">None,</span><span style="color:#82AAFF;"> </span><span style="color:#676E95;font-style:italic;"># And LoftQ</span></span>
<span class="line"><span style="color:#89DDFF;">)</span></span>
<span class="line"></span></code></pre></div><ol start="4"><li>加载并构造 train-data</li></ol><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#A6ACCD;">alpaca_prompt </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">&quot;&quot;&quot;</span><span style="color:#C3E88D;">Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.</span></span>
<span class="line"></span>
<span class="line"><span style="color:#C3E88D;">### Instruction:</span></span>
<span class="line"><span style="color:#F78C6C;">{}</span></span>
<span class="line"></span>
<span class="line"><span style="color:#C3E88D;">### Input:</span></span>
<span class="line"><span style="color:#F78C6C;">{}</span></span>
<span class="line"></span>
<span class="line"><span style="color:#C3E88D;">### Response:</span></span>
<span class="line"><span style="color:#F78C6C;">{}</span><span style="color:#89DDFF;">&quot;&quot;&quot;</span></span>
<span class="line"></span>
<span class="line"><span style="color:#A6ACCD;">EOS_TOKEN </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> tokenizer</span><span style="color:#89DDFF;">.</span><span style="color:#F07178;">eos_token</span><span style="color:#A6ACCD;"> </span><span style="color:#676E95;font-style:italic;"># Must add EOS_TOKEN</span></span>
<span class="line"><span style="color:#C792EA;">def</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">formatting_prompts_func</span><span style="color:#89DDFF;">(</span><span style="color:#A6ACCD;font-style:italic;">examples</span><span style="color:#89DDFF;">):</span></span>
<span class="line"><span style="color:#A6ACCD;">    instructions </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> examples</span><span style="color:#89DDFF;">[</span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">instruction</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">]</span></span>
<span class="line"><span style="color:#A6ACCD;">    inputs       </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> examples</span><span style="color:#89DDFF;">[</span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">input</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">]</span></span>
<span class="line"><span style="color:#A6ACCD;">    outputs      </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> examples</span><span style="color:#89DDFF;">[</span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">output</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">]</span></span>
<span class="line"><span style="color:#A6ACCD;">    texts </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">[]</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#89DDFF;font-style:italic;">for</span><span style="color:#A6ACCD;"> instruction</span><span style="color:#89DDFF;">,</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">input</span><span style="color:#89DDFF;">,</span><span style="color:#A6ACCD;"> output </span><span style="color:#89DDFF;font-style:italic;">in</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">zip</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">instructions</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> inputs</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> outputs</span><span style="color:#89DDFF;">):</span></span>
<span class="line"><span style="color:#A6ACCD;">        </span><span style="color:#676E95;font-style:italic;"># Must add EOS_TOKEN, otherwise your generation will go on forever!</span></span>
<span class="line"><span style="color:#A6ACCD;">        text </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> alpaca_prompt</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">format</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">instruction</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> input</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> output</span><span style="color:#89DDFF;">)</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">+</span><span style="color:#A6ACCD;"> EOS_TOKEN</span></span>
<span class="line"><span style="color:#A6ACCD;">        texts</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">append</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">text</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#89DDFF;font-style:italic;">return</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">{</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">text</span><span style="color:#89DDFF;">&quot;</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">:</span><span style="color:#A6ACCD;"> texts</span><span style="color:#89DDFF;">,</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">}</span></span>
<span class="line"><span style="color:#89DDFF;font-style:italic;">pass</span></span>
<span class="line"></span>
<span class="line"><span style="color:#89DDFF;font-style:italic;">from</span><span style="color:#A6ACCD;"> datasets </span><span style="color:#89DDFF;font-style:italic;">import</span><span style="color:#A6ACCD;"> load_dataset</span></span>
<span class="line"><span style="color:#A6ACCD;">dataset </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">load_dataset</span><span style="color:#89DDFF;">(</span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">yahma/alpaca-cleaned</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#A6ACCD;font-style:italic;">split</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">=</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">train</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#A6ACCD;">dataset </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> dataset</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">map</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">formatting_prompts_func</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#A6ACCD;font-style:italic;">batched</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">=</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">True,)</span></span>
<span class="line"></span></code></pre></div><ol start="5"><li>训练模型配置</li></ol><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#89DDFF;font-style:italic;">from</span><span style="color:#A6ACCD;"> trl </span><span style="color:#89DDFF;font-style:italic;">import</span><span style="color:#A6ACCD;"> SFTTrainer</span></span>
<span class="line"><span style="color:#89DDFF;font-style:italic;">from</span><span style="color:#A6ACCD;"> transformers </span><span style="color:#89DDFF;font-style:italic;">import</span><span style="color:#A6ACCD;"> TrainingArguments</span></span>
<span class="line"><span style="color:#89DDFF;font-style:italic;">from</span><span style="color:#A6ACCD;"> unsloth </span><span style="color:#89DDFF;font-style:italic;">import</span><span style="color:#A6ACCD;"> is_bfloat16_supported</span></span>
<span class="line"></span>
<span class="line"><span style="color:#A6ACCD;">trainer </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">SFTTrainer</span><span style="color:#89DDFF;">(</span></span>
<span class="line"><span style="color:#82AAFF;">    </span><span style="color:#A6ACCD;font-style:italic;">model</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">=</span><span style="color:#82AAFF;"> model</span><span style="color:#89DDFF;">,</span></span>
<span class="line"><span style="color:#82AAFF;">    </span><span style="color:#A6ACCD;font-style:italic;">tokenizer</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">=</span><span style="color:#82AAFF;"> tokenizer</span><span style="color:#89DDFF;">,</span></span>
<span class="line"><span style="color:#82AAFF;">    </span><span style="color:#A6ACCD;font-style:italic;">train_dataset</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">=</span><span style="color:#82AAFF;"> dataset</span><span style="color:#89DDFF;">,</span></span>
<span class="line"><span style="color:#82AAFF;">    </span><span style="color:#A6ACCD;font-style:italic;">dataset_text_field</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">=</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">text</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">,</span></span>
<span class="line"><span style="color:#82AAFF;">    </span><span style="color:#A6ACCD;font-style:italic;">max_seq_length</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">=</span><span style="color:#82AAFF;"> max_seq_length</span><span style="color:#89DDFF;">,</span></span>
<span class="line"><span style="color:#82AAFF;">    </span><span style="color:#A6ACCD;font-style:italic;">dataset_num_proc</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">=</span><span style="color:#82AAFF;"> </span><span style="color:#F78C6C;">2</span><span style="color:#89DDFF;">,</span></span>
<span class="line"><span style="color:#82AAFF;">    </span><span style="color:#A6ACCD;font-style:italic;">packing</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">=</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">False,</span><span style="color:#82AAFF;"> </span><span style="color:#676E95;font-style:italic;"># Can make training 5x faster for short sequences.</span></span>
<span class="line"><span style="color:#82AAFF;">    </span><span style="color:#A6ACCD;font-style:italic;">args</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">=</span><span style="color:#82AAFF;"> TrainingArguments</span><span style="color:#89DDFF;">(</span></span>
<span class="line"><span style="color:#82AAFF;">        </span><span style="color:#A6ACCD;font-style:italic;">per_device_train_batch_size</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">=</span><span style="color:#82AAFF;"> </span><span style="color:#F78C6C;">2</span><span style="color:#89DDFF;">,</span></span>
<span class="line"><span style="color:#82AAFF;">        </span><span style="color:#A6ACCD;font-style:italic;">gradient_accumulation_steps</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">=</span><span style="color:#82AAFF;"> </span><span style="color:#F78C6C;">4</span><span style="color:#89DDFF;">,</span></span>
<span class="line"><span style="color:#82AAFF;">        </span><span style="color:#A6ACCD;font-style:italic;">warmup_steps</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">=</span><span style="color:#82AAFF;"> </span><span style="color:#F78C6C;">5</span><span style="color:#89DDFF;">,</span></span>
<span class="line"><span style="color:#82AAFF;">        </span><span style="color:#A6ACCD;font-style:italic;">max_steps</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">=</span><span style="color:#82AAFF;"> </span><span style="color:#F78C6C;">60</span><span style="color:#89DDFF;">,</span></span>
<span class="line"><span style="color:#82AAFF;">        </span><span style="color:#A6ACCD;font-style:italic;">learning_rate</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">=</span><span style="color:#82AAFF;"> </span><span style="color:#F78C6C;">2e-4</span><span style="color:#89DDFF;">,</span></span>
<span class="line"><span style="color:#82AAFF;">        </span><span style="color:#A6ACCD;font-style:italic;">fp16</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">=</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;font-style:italic;">not</span><span style="color:#82AAFF;"> is_bfloat16_supported</span><span style="color:#89DDFF;">(),</span></span>
<span class="line"><span style="color:#82AAFF;">        </span><span style="color:#A6ACCD;font-style:italic;">bf16</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">=</span><span style="color:#82AAFF;"> is_bfloat16_supported</span><span style="color:#89DDFF;">(),</span></span>
<span class="line"><span style="color:#82AAFF;">        </span><span style="color:#A6ACCD;font-style:italic;">logging_steps</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">=</span><span style="color:#82AAFF;"> </span><span style="color:#F78C6C;">1</span><span style="color:#89DDFF;">,</span></span>
<span class="line"><span style="color:#82AAFF;">        </span><span style="color:#A6ACCD;font-style:italic;">optim</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">=</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">adamw_8bit</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">,</span></span>
<span class="line"><span style="color:#82AAFF;">        </span><span style="color:#A6ACCD;font-style:italic;">weight_decay</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">=</span><span style="color:#82AAFF;"> </span><span style="color:#F78C6C;">0.01</span><span style="color:#89DDFF;">,</span></span>
<span class="line"><span style="color:#82AAFF;">        </span><span style="color:#A6ACCD;font-style:italic;">lr_scheduler_type</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">=</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">linear</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">,</span></span>
<span class="line"><span style="color:#82AAFF;">        </span><span style="color:#A6ACCD;font-style:italic;">seed</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">=</span><span style="color:#82AAFF;"> </span><span style="color:#F78C6C;">3407</span><span style="color:#89DDFF;">,</span></span>
<span class="line"><span style="color:#82AAFF;">        </span><span style="color:#A6ACCD;font-style:italic;">output_dir</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">=</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">outputs</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">,</span></span>
<span class="line"><span style="color:#82AAFF;">    </span><span style="color:#89DDFF;">),</span></span>
<span class="line"><span style="color:#89DDFF;">)</span></span>
<span class="line"></span></code></pre></div><ol start="6"><li>[Optional]显示当前memory状态[对比内存占用]</li></ol><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#676E95;font-style:italic;">#@title Show current memory stats</span></span>
<span class="line"><span style="color:#A6ACCD;">gpu_stats </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> torch</span><span style="color:#89DDFF;">.</span><span style="color:#F07178;">cuda</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">get_device_properties</span><span style="color:#89DDFF;">(</span><span style="color:#F78C6C;">0</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#A6ACCD;">start_gpu_memory </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">round</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">torch</span><span style="color:#89DDFF;">.</span><span style="color:#F07178;">cuda</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">max_memory_reserved</span><span style="color:#89DDFF;">()</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">/</span><span style="color:#82AAFF;"> </span><span style="color:#F78C6C;">1024</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">/</span><span style="color:#82AAFF;"> </span><span style="color:#F78C6C;">1024</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">/</span><span style="color:#82AAFF;"> </span><span style="color:#F78C6C;">1024</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#F78C6C;">3</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#A6ACCD;">max_memory </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">round</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">gpu_stats</span><span style="color:#89DDFF;">.</span><span style="color:#F07178;">total_memory</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">/</span><span style="color:#82AAFF;"> </span><span style="color:#F78C6C;">1024</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">/</span><span style="color:#82AAFF;"> </span><span style="color:#F78C6C;">1024</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">/</span><span style="color:#82AAFF;"> </span><span style="color:#F78C6C;">1024</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#F78C6C;">3</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#82AAFF;">print</span><span style="color:#89DDFF;">(</span><span style="color:#C792EA;">f</span><span style="color:#C3E88D;">&quot;GPU = </span><span style="color:#F78C6C;">{</span><span style="color:#82AAFF;">gpu_stats</span><span style="color:#89DDFF;">.</span><span style="color:#F07178;">name</span><span style="color:#F78C6C;">}</span><span style="color:#C3E88D;">. Max memory = </span><span style="color:#F78C6C;">{</span><span style="color:#82AAFF;">max_memory</span><span style="color:#F78C6C;">}</span><span style="color:#C3E88D;"> GB.&quot;</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#82AAFF;">print</span><span style="color:#89DDFF;">(</span><span style="color:#C792EA;">f</span><span style="color:#C3E88D;">&quot;</span><span style="color:#F78C6C;">{</span><span style="color:#82AAFF;">start_gpu_memory</span><span style="color:#F78C6C;">}</span><span style="color:#C3E88D;"> GB of memory reserved.&quot;</span><span style="color:#89DDFF;">)</span></span>
<span class="line"></span></code></pre></div><ol start="7"><li>实际执行训练</li></ol><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#A6ACCD;">trainer_stats </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> trainer</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">train</span><span style="color:#89DDFF;">()</span></span>
<span class="line"></span></code></pre></div><ol start="8"><li>[Optional]显示当前memory状态[对比内存占用]</li></ol><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#676E95;font-style:italic;">#@title Show final memory and time stats</span></span>
<span class="line"><span style="color:#A6ACCD;">used_memory </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">round</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">torch</span><span style="color:#89DDFF;">.</span><span style="color:#F07178;">cuda</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">max_memory_reserved</span><span style="color:#89DDFF;">()</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">/</span><span style="color:#82AAFF;"> </span><span style="color:#F78C6C;">1024</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">/</span><span style="color:#82AAFF;"> </span><span style="color:#F78C6C;">1024</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">/</span><span style="color:#82AAFF;"> </span><span style="color:#F78C6C;">1024</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#F78C6C;">3</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#A6ACCD;">used_memory_for_lora </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">round</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">used_memory </span><span style="color:#89DDFF;">-</span><span style="color:#82AAFF;"> start_gpu_memory</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#F78C6C;">3</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#A6ACCD;">used_percentage </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">round</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">used_memory         </span><span style="color:#89DDFF;">/</span><span style="color:#82AAFF;">max_memory</span><span style="color:#89DDFF;">*</span><span style="color:#F78C6C;">100</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#F78C6C;">3</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#A6ACCD;">lora_percentage </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">round</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">used_memory_for_lora</span><span style="color:#89DDFF;">/</span><span style="color:#82AAFF;">max_memory</span><span style="color:#89DDFF;">*</span><span style="color:#F78C6C;">100</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#F78C6C;">3</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#82AAFF;">print</span><span style="color:#89DDFF;">(</span><span style="color:#C792EA;">f</span><span style="color:#C3E88D;">&quot;</span><span style="color:#F78C6C;">{</span><span style="color:#82AAFF;">trainer_stats</span><span style="color:#89DDFF;">.</span><span style="color:#F07178;">metrics</span><span style="color:#89DDFF;">[</span><span style="color:#89DDFF;">&#39;</span><span style="color:#C3E88D;">train_runtime</span><span style="color:#89DDFF;">&#39;</span><span style="color:#89DDFF;">]</span><span style="color:#F78C6C;">}</span><span style="color:#C3E88D;"> seconds used for training.&quot;</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#82AAFF;">print</span><span style="color:#89DDFF;">(</span><span style="color:#C792EA;">f</span><span style="color:#C3E88D;">&quot;</span><span style="color:#F78C6C;">{</span><span style="color:#82AAFF;">round</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">trainer_stats</span><span style="color:#89DDFF;">.</span><span style="color:#F07178;">metrics</span><span style="color:#89DDFF;">[</span><span style="color:#89DDFF;">&#39;</span><span style="color:#C3E88D;">train_runtime</span><span style="color:#89DDFF;">&#39;</span><span style="color:#89DDFF;">]/</span><span style="color:#F78C6C;">60</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#F78C6C;">2</span><span style="color:#89DDFF;">)</span><span style="color:#F78C6C;">}</span><span style="color:#C3E88D;"> minutes used for training.&quot;</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#82AAFF;">print</span><span style="color:#89DDFF;">(</span><span style="color:#C792EA;">f</span><span style="color:#C3E88D;">&quot;Peak reserved memory = </span><span style="color:#F78C6C;">{</span><span style="color:#82AAFF;">used_memory</span><span style="color:#F78C6C;">}</span><span style="color:#C3E88D;"> GB.&quot;</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#82AAFF;">print</span><span style="color:#89DDFF;">(</span><span style="color:#C792EA;">f</span><span style="color:#C3E88D;">&quot;Peak reserved memory for training = </span><span style="color:#F78C6C;">{</span><span style="color:#82AAFF;">used_memory_for_lora</span><span style="color:#F78C6C;">}</span><span style="color:#C3E88D;"> GB.&quot;</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#82AAFF;">print</span><span style="color:#89DDFF;">(</span><span style="color:#C792EA;">f</span><span style="color:#C3E88D;">&quot;Peak reserved memory % of max memory = </span><span style="color:#F78C6C;">{</span><span style="color:#82AAFF;">used_percentage</span><span style="color:#F78C6C;">}</span><span style="color:#C3E88D;"> %.&quot;</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#82AAFF;">print</span><span style="color:#89DDFF;">(</span><span style="color:#C792EA;">f</span><span style="color:#C3E88D;">&quot;Peak reserved memory for training % of max memory = </span><span style="color:#F78C6C;">{</span><span style="color:#82AAFF;">lora_percentage</span><span style="color:#F78C6C;">}</span><span style="color:#C3E88D;"> %.&quot;</span><span style="color:#89DDFF;">)</span></span>
<span class="line"></span></code></pre></div><ol start="9"><li>完成后执行推理</li></ol><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#676E95;font-style:italic;"># alpaca_prompt = Copied from above</span></span>
<span class="line"><span style="color:#A6ACCD;">FastLanguageModel</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">for_inference</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">model</span><span style="color:#89DDFF;">)</span><span style="color:#A6ACCD;"> </span><span style="color:#676E95;font-style:italic;"># Enable native 2x faster inference</span></span>
<span class="line"><span style="color:#A6ACCD;">inputs </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">tokenizer</span><span style="color:#89DDFF;">(</span></span>
<span class="line"><span style="color:#89DDFF;">[</span></span>
<span class="line"><span style="color:#82AAFF;">    alpaca_prompt</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">format</span><span style="color:#89DDFF;">(</span></span>
<span class="line"><span style="color:#82AAFF;">        </span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">Continue the fibonnaci sequence.</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#676E95;font-style:italic;"># instruction</span></span>
<span class="line"><span style="color:#82AAFF;">        </span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">1, 1, 2, 3, 5, 8</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#676E95;font-style:italic;"># input</span></span>
<span class="line"><span style="color:#82AAFF;">        </span><span style="color:#89DDFF;">&quot;&quot;</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#676E95;font-style:italic;"># output - leave this blank for generation!</span></span>
<span class="line"><span style="color:#82AAFF;">    </span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#89DDFF;">],</span><span style="color:#82AAFF;"> </span><span style="color:#A6ACCD;font-style:italic;">return_tensors</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">=</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">pt</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">).</span><span style="color:#82AAFF;">to</span><span style="color:#89DDFF;">(</span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">cuda</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#A6ACCD;">outputs </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> model</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">generate</span><span style="color:#89DDFF;">(**</span><span style="color:#82AAFF;">inputs</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#A6ACCD;font-style:italic;">max_new_tokens</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">=</span><span style="color:#82AAFF;"> </span><span style="color:#F78C6C;">64</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#A6ACCD;font-style:italic;">use_cache</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">=</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">True)</span></span>
<span class="line"><span style="color:#A6ACCD;">tokenizer</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">batch_decode</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">outputs</span><span style="color:#89DDFF;">)</span></span>
<span class="line"></span></code></pre></div><p><code>stream</code>形式</p><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#676E95;font-style:italic;"># alpaca_prompt = Copied from above</span></span>
<span class="line"><span style="color:#A6ACCD;">FastLanguageModel</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">for_inference</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">model</span><span style="color:#89DDFF;">)</span><span style="color:#A6ACCD;"> </span><span style="color:#676E95;font-style:italic;"># Enable native 2x faster inference</span></span>
<span class="line"><span style="color:#A6ACCD;">inputs </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">tokenizer</span><span style="color:#89DDFF;">(</span></span>
<span class="line"><span style="color:#89DDFF;">[</span></span>
<span class="line"><span style="color:#82AAFF;">    alpaca_prompt</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">format</span><span style="color:#89DDFF;">(</span></span>
<span class="line"><span style="color:#82AAFF;">        </span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">Continue the fibonnaci sequence.</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#676E95;font-style:italic;"># instruction</span></span>
<span class="line"><span style="color:#82AAFF;">        </span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">1, 1, 2, 3, 5, 8</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#676E95;font-style:italic;"># input</span></span>
<span class="line"><span style="color:#82AAFF;">        </span><span style="color:#89DDFF;">&quot;&quot;</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#676E95;font-style:italic;"># output - leave this blank for generation!</span></span>
<span class="line"><span style="color:#82AAFF;">    </span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#89DDFF;">],</span><span style="color:#82AAFF;"> </span><span style="color:#A6ACCD;font-style:italic;">return_tensors</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">=</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">pt</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">).</span><span style="color:#82AAFF;">to</span><span style="color:#89DDFF;">(</span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">cuda</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#89DDFF;font-style:italic;">from</span><span style="color:#A6ACCD;"> transformers </span><span style="color:#89DDFF;font-style:italic;">import</span><span style="color:#A6ACCD;"> TextStreamer</span></span>
<span class="line"><span style="color:#A6ACCD;">text_streamer </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">TextStreamer</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">tokenizer</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#A6ACCD;">_ </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> model</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">generate</span><span style="color:#89DDFF;">(**</span><span style="color:#82AAFF;">inputs</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#A6ACCD;font-style:italic;">streamer</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">=</span><span style="color:#82AAFF;"> text_streamer</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#A6ACCD;font-style:italic;">max_new_tokens</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">=</span><span style="color:#82AAFF;"> </span><span style="color:#F78C6C;">128</span><span style="color:#89DDFF;">)</span></span>
<span class="line"></span></code></pre></div><ol start="10"><li>保存模型</li></ol><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#A6ACCD;">model</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">save_pretrained</span><span style="color:#89DDFF;">(</span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">lora_model</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">)</span><span style="color:#A6ACCD;"> </span><span style="color:#676E95;font-style:italic;"># Local saving</span></span>
<span class="line"><span style="color:#A6ACCD;">tokenizer</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">save_pretrained</span><span style="color:#89DDFF;">(</span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">lora_model</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;"># model.push_to_hub(&quot;your_name/lora_model&quot;, token = &quot;...&quot;) # Online saving</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;"># tokenizer.push_to_hub(&quot;your_name/lora_model&quot;, token = &quot;...&quot;) # Online saving</span></span>
<span class="line"></span></code></pre></div><ol start="11"><li>unsloth加载模型</li></ol><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#89DDFF;font-style:italic;">if</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">False:</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#89DDFF;font-style:italic;">from</span><span style="color:#A6ACCD;"> unsloth </span><span style="color:#89DDFF;font-style:italic;">import</span><span style="color:#A6ACCD;"> FastLanguageModel</span></span>
<span class="line"><span style="color:#A6ACCD;">    model</span><span style="color:#89DDFF;">,</span><span style="color:#A6ACCD;"> tokenizer </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> FastLanguageModel</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">from_pretrained</span><span style="color:#89DDFF;">(</span></span>
<span class="line"><span style="color:#82AAFF;">        </span><span style="color:#A6ACCD;font-style:italic;">model_name</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">=</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">lora_model</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#676E95;font-style:italic;"># YOUR MODEL YOU USED FOR TRAINING</span></span>
<span class="line"><span style="color:#82AAFF;">        </span><span style="color:#A6ACCD;font-style:italic;">max_seq_length</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">=</span><span style="color:#82AAFF;"> max_seq_length</span><span style="color:#89DDFF;">,</span></span>
<span class="line"><span style="color:#82AAFF;">        </span><span style="color:#A6ACCD;font-style:italic;">dtype</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">=</span><span style="color:#82AAFF;"> dtype</span><span style="color:#89DDFF;">,</span></span>
<span class="line"><span style="color:#82AAFF;">        </span><span style="color:#A6ACCD;font-style:italic;">load_in_4bit</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">=</span><span style="color:#82AAFF;"> load_in_4bit</span><span style="color:#89DDFF;">,</span></span>
<span class="line"><span style="color:#82AAFF;">    </span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#A6ACCD;">    FastLanguageModel</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">for_inference</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">model</span><span style="color:#89DDFF;">)</span><span style="color:#A6ACCD;"> </span><span style="color:#676E95;font-style:italic;"># Enable native 2x faster inference</span></span>
<span class="line"></span>
<span class="line"><span style="color:#676E95;font-style:italic;"># alpaca_prompt = You MUST copy from above!</span></span>
<span class="line"></span>
<span class="line"><span style="color:#A6ACCD;">inputs </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">tokenizer</span><span style="color:#89DDFF;">(</span></span>
<span class="line"><span style="color:#89DDFF;">[</span></span>
<span class="line"><span style="color:#82AAFF;">    alpaca_prompt</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">format</span><span style="color:#89DDFF;">(</span></span>
<span class="line"><span style="color:#82AAFF;">        </span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">What is a famous tall tower in Paris?</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#676E95;font-style:italic;"># instruction</span></span>
<span class="line"><span style="color:#82AAFF;">        </span><span style="color:#89DDFF;">&quot;&quot;</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#676E95;font-style:italic;"># input</span></span>
<span class="line"><span style="color:#82AAFF;">        </span><span style="color:#89DDFF;">&quot;&quot;</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#676E95;font-style:italic;"># output - leave this blank for generation!</span></span>
<span class="line"><span style="color:#82AAFF;">    </span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#89DDFF;">],</span><span style="color:#82AAFF;"> </span><span style="color:#A6ACCD;font-style:italic;">return_tensors</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">=</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">pt</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">).</span><span style="color:#82AAFF;">to</span><span style="color:#89DDFF;">(</span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">cuda</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#A6ACCD;">outputs </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> model</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">generate</span><span style="color:#89DDFF;">(**</span><span style="color:#82AAFF;">inputs</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#A6ACCD;font-style:italic;">max_new_tokens</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">=</span><span style="color:#82AAFF;"> </span><span style="color:#F78C6C;">64</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#A6ACCD;font-style:italic;">use_cache</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">=</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">True)</span></span>
<span class="line"><span style="color:#A6ACCD;">tokenizer</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">batch_decode</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">outputs</span><span style="color:#89DDFF;">)</span></span>
<span class="line"></span></code></pre></div><ol start="12"><li>hf加载模型</li></ol><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#89DDFF;font-style:italic;">if</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">False:</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#676E95;font-style:italic;"># I highly do NOT suggest - use Unsloth if possible</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#89DDFF;font-style:italic;">from</span><span style="color:#A6ACCD;"> peft </span><span style="color:#89DDFF;font-style:italic;">import</span><span style="color:#A6ACCD;"> AutoPeftModelForCausalLM</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#89DDFF;font-style:italic;">from</span><span style="color:#A6ACCD;"> transformers </span><span style="color:#89DDFF;font-style:italic;">import</span><span style="color:#A6ACCD;"> AutoTokenizer</span></span>
<span class="line"><span style="color:#A6ACCD;">    model </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> AutoPeftModelForCausalLM</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">from_pretrained</span><span style="color:#89DDFF;">(</span></span>
<span class="line"><span style="color:#82AAFF;">        </span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">lora_model</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#676E95;font-style:italic;"># YOUR MODEL YOU USED FOR TRAINING</span></span>
<span class="line"><span style="color:#82AAFF;">        </span><span style="color:#A6ACCD;font-style:italic;">load_in_4bit</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">=</span><span style="color:#82AAFF;"> load_in_4bit</span><span style="color:#89DDFF;">,</span></span>
<span class="line"><span style="color:#82AAFF;">    </span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#A6ACCD;">    tokenizer </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> AutoTokenizer</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">from_pretrained</span><span style="color:#89DDFF;">(</span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">lora_model</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">)</span></span>
<span class="line"></span></code></pre></div><ol start="13"><li>保存为VLLM float16</li></ol><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#676E95;font-style:italic;"># Merge to 16bit</span></span>
<span class="line"><span style="color:#89DDFF;font-style:italic;">if</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">True:</span><span style="color:#A6ACCD;"> model</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">save_pretrained_merged</span><span style="color:#89DDFF;">(</span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">llama-3-8b-bnb-4bit-ft-1</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> tokenizer</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#A6ACCD;font-style:italic;">save_method</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">=</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">merged_16bit</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">,)</span></span>
<span class="line"><span style="color:#89DDFF;font-style:italic;">if</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">True:</span><span style="color:#A6ACCD;"> model</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">push_to_hub_merged</span><span style="color:#89DDFF;">(</span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">lumiseven/llama-3-8b-bnb-4bit-ft-1</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> tokenizer</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#A6ACCD;font-style:italic;">save_method</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">=</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">merged_16bit</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#A6ACCD;font-style:italic;">token</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">=</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">&quot;&quot;</span><span style="color:#89DDFF;">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#676E95;font-style:italic;"># Merge to 4bit</span></span>
<span class="line"><span style="color:#89DDFF;font-style:italic;">if</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">False:</span><span style="color:#A6ACCD;"> model</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">save_pretrained_merged</span><span style="color:#89DDFF;">(</span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">llama-3-8b-bnb-4bit-ft-1</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> tokenizer</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#A6ACCD;font-style:italic;">save_method</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">=</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">merged_4bit</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">,)</span></span>
<span class="line"><span style="color:#89DDFF;font-style:italic;">if</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">False:</span><span style="color:#A6ACCD;"> model</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">push_to_hub_merged</span><span style="color:#89DDFF;">(</span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">lumiseven/llama-3-8b-bnb-4bit-ft-1</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> tokenizer</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#A6ACCD;font-style:italic;">save_method</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">=</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">merged_4bit</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#A6ACCD;font-style:italic;">token</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">=</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">&quot;&quot;</span><span style="color:#89DDFF;">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#676E95;font-style:italic;"># Just LoRA adapters</span></span>
<span class="line"><span style="color:#89DDFF;font-style:italic;">if</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">False:</span><span style="color:#A6ACCD;"> model</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">save_pretrained_merged</span><span style="color:#89DDFF;">(</span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">model</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> tokenizer</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#A6ACCD;font-style:italic;">save_method</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">=</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">lora</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">,)</span></span>
<span class="line"><span style="color:#89DDFF;font-style:italic;">if</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">False:</span><span style="color:#A6ACCD;"> model</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">push_to_hub_merged</span><span style="color:#89DDFF;">(</span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">hf/model</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> tokenizer</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#A6ACCD;font-style:italic;">save_method</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">=</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">lora</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#A6ACCD;font-style:italic;">token</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">=</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">&quot;&quot;</span><span style="color:#89DDFF;">)</span></span>
<span class="line"></span></code></pre></div><ol start="14"><li>GGUF/llama.cpp 转换</li></ol><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#676E95;font-style:italic;"># Save to 8bit Q8_0</span></span>
<span class="line"><span style="color:#89DDFF;font-style:italic;">if</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">False:</span><span style="color:#A6ACCD;"> model</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">save_pretrained_gguf</span><span style="color:#89DDFF;">(</span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">model</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> tokenizer</span><span style="color:#89DDFF;">,)</span></span>
<span class="line"><span style="color:#89DDFF;font-style:italic;">if</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">False:</span><span style="color:#A6ACCD;"> model</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">push_to_hub_gguf</span><span style="color:#89DDFF;">(</span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">hf/model</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> tokenizer</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#A6ACCD;font-style:italic;">token</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">=</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">&quot;&quot;</span><span style="color:#89DDFF;">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#676E95;font-style:italic;"># Save to 16bit GGUF</span></span>
<span class="line"><span style="color:#89DDFF;font-style:italic;">if</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">False:</span><span style="color:#A6ACCD;"> model</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">save_pretrained_gguf</span><span style="color:#89DDFF;">(</span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">model</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> tokenizer</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#A6ACCD;font-style:italic;">quantization_method</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">=</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">f16</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#89DDFF;font-style:italic;">if</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">False:</span><span style="color:#A6ACCD;"> model</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">push_to_hub_gguf</span><span style="color:#89DDFF;">(</span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">hf/model</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> tokenizer</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#A6ACCD;font-style:italic;">quantization_method</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">=</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">f16</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#A6ACCD;font-style:italic;">token</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">=</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">&quot;&quot;</span><span style="color:#89DDFF;">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#676E95;font-style:italic;"># Save to q4_k_m GGUF</span></span>
<span class="line"><span style="color:#89DDFF;font-style:italic;">if</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">False:</span><span style="color:#A6ACCD;"> model</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">save_pretrained_gguf</span><span style="color:#89DDFF;">(</span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">model</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> tokenizer</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#A6ACCD;font-style:italic;">quantization_method</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">=</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">q4_k_m</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#89DDFF;font-style:italic;">if</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">False:</span><span style="color:#A6ACCD;"> model</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">push_to_hub_gguf</span><span style="color:#89DDFF;">(</span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">hf/model</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> tokenizer</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#A6ACCD;font-style:italic;">quantization_method</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">=</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">q4_k_m</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#A6ACCD;font-style:italic;">token</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">=</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">&quot;&quot;</span><span style="color:#89DDFF;">)</span></span>
<span class="line"></span></code></pre></div>`,41),t=[p];function e(F,c,r,y,D,A){return a(),n("div",null,t)}const u=s(o,[["render",e]]);export{C as __pageData,u as default};
